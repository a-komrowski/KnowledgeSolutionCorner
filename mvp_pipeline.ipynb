{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a41858",
   "metadata": {},
   "source": [
    "# MVP-Pipeline: Dataland ‚Üí Transform ‚Üí Mock-Rooms ‚Üí AI & Visualisierung\n",
    "\n",
    "In diesem Notebook bauen wir Schritt f√ºr Schritt eine Daten-Pipeline, die:\n",
    "1. **Nachhaltigkeitsdaten** von Unternehmen aus der Dataland-API holt\n",
    "2. Diese Daten **transformiert** und in ein einheitliches Format bringt\n",
    "3. Die Daten in **Mock-Rooms** (lokale Dateien) speichert\n",
    "4. **KI-basierte Analysen** und **Visualisierungen** erm√∂glicht\n",
    "\n",
    "---\n",
    "\n",
    "### Was ist Dataland?\n",
    "\n",
    "Dataland ist eine Plattform, die ESG-Daten (Environmental, Social, Governance) von Unternehmen sammelt und √ºber eine API bereitstellt. ESG-Daten umfassen z.B.:\n",
    "- CO‚ÇÇ-Emissionen\n",
    "- Energieverbrauch\n",
    "- Mitarbeiter-Kennzahlen\n",
    "- Governance-Strukturen\n",
    "\n",
    "### Was ist ein Connector?\n",
    "\n",
    "Ein Connector ist ein Programmteil, der:\n",
    "- Eine **Verbindung** zu einer externen Datenquelle aufbaut\n",
    "- Die Daten **abruft** (meist √ºber HTTP-Requests)\n",
    "- Die Rohdaten f√ºr die weitere Verarbeitung **bereitstellt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31729662",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Konfiguration\n",
    "\n",
    "Zuerst installieren und importieren wir alle ben√∂tigten Python-Bibliotheken.\n",
    "\n",
    "### Was machen diese Bibliotheken?\n",
    "\n",
    "- **requests**: Erm√∂glicht HTTP-Anfragen an APIs (wie Dataland)\n",
    "- **json**: Verarbeitet JSON-Daten (das Standard-Datenformat von APIs)\n",
    "- **datetime**: Hilft beim Arbeiten mit Datum und Zeit\n",
    "- **uuid**: Erzeugt eindeutige IDs f√ºr unsere Events\n",
    "- **os**: Erm√∂glicht Zugriff auf das Dateisystem\n",
    "- **pathlib**: Moderner Weg, um mit Dateipfaden zu arbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c5316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Alle Bibliotheken erfolgreich importiert!\n",
      "‚úÖ Umgebungsvariablen aus .env geladen!\n"
     ]
    }
   ],
   "source": [
    "# Bibliotheken importieren\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "# Lade Umgebungsvariablen aus .env-Datei\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Alle Bibliotheken erfolgreich importiert!\")\n",
    "print(\"‚úÖ Umgebungsvariablen aus .env geladen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0abe51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Konfiguration der Dataland-API\n",
    "\n",
    "Bevor wir Daten abrufen k√∂nnen, m√ºssen wir festlegen:\n",
    "- Wo ist die API? (Base URL)\n",
    "- Wie authentifizieren wir uns? (API Key)\n",
    "- Welches Unternehmen interessiert uns?\n",
    "\n",
    "### Wichtig: API-Zugang\n",
    "\n",
    "In diesem MVP verwenden wir zun√§chst **Mock-Daten** oder √∂ffentlich zug√§ngliche Endpoints. F√ºr den Produktivbetrieb w√ºrden Sie:\n",
    "1. Einen Account bei Dataland erstellen\n",
    "2. Einen API-Key erhalten\n",
    "3. Diesen sicher in einer Umgebungsvariable speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd967cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Konfiguration (alles an einem Ort)\u001b[39;00m\n\u001b[32m      2\u001b[39m CONFIG = {\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# API Base URLs (getrennte Services!)\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbase_url_api\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhttps://dataland.com/api\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbase_url_documents\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhttps://dataland.com/documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# API Token aus Environment\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapi_token\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mos\u001b[49m.getenv(\u001b[33m\"\u001b[39m\u001b[33mDATALAND_TOKEN\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Unternehmen zum Testen\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompany_query\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mSiemens\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Company-Lookup Kandidaten (API-Service)\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Probiere verschiedene Endpunkte, da √∂ffentliche Swagger nicht eindeutig ist\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompany_lookup_api_candidates\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# {\"path\": \"/metadata/companies/search\", \"params\": {\"q\": None}},\u001b[39;00m\n\u001b[32m     17\u001b[39m         \u001b[38;5;66;03m# {\"path\": \"/metadata/companies\", \"params\": {\"query\": None}},\u001b[39;00m\n\u001b[32m     18\u001b[39m         \u001b[38;5;66;03m# {\"path\": \"/companies/search\", \"params\": {\"q\": None}},\u001b[39;00m\n\u001b[32m     19\u001b[39m         \u001b[38;5;66;03m# {\"path\": \"/companies\", \"params\": {\"name\": None}},\u001b[39;00m\n\u001b[32m     20\u001b[39m         \u001b[38;5;66;03m# {\"path\": \"/entities/search\", \"params\": {\"q\": None}},\u001b[39;00m\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# {\"path\": \"/api/companies/search\", \"params\": {\"q\": None}},\u001b[39;00m\n\u001b[32m     22\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/companies/names\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33msearchString\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     23\u001b[39m     ],\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Company-Lookup Kandidaten (Documents-Service) - Fallback\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompany_lookup_doc_candidates\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     27\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     28\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/search\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     29\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     30\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/search\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     31\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/companies/search\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     32\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m/documents/search\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}},\n\u001b[32m     33\u001b[39m     ],\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Timeouts\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout_search\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m30\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m20\u001b[39m,\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Retry-Konfiguration\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbackoff_base\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m,        \u001b[38;5;66;03m# Exponential Backoff Basis (2^retry_count)\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbackoff_max\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m60\u001b[39m,        \u001b[38;5;66;03m# Maximale Wartezeit in Sekunden\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrate_limit_wait\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m60\u001b[39m,    \u001b[38;5;66;03m# Wartezeit bei Rate Limit (429)\u001b[39;00m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Raw Data Directory\u001b[39;00m\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mraw_dir\u001b[39m\u001b[33m\"\u001b[39m: Path(\u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m }\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Erstelle raw/ Verzeichnis\u001b[39;00m\n\u001b[32m     50\u001b[39m CONFIG[\u001b[33m\"\u001b[39m\u001b[33mraw_dir\u001b[39m\u001b[33m\"\u001b[39m].mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Konfiguration (alles an einem Ort)\n",
    "CONFIG = {\n",
    "    # API Base URLs (getrennte Services!)\n",
    "    \"base_url_api\": \"https://dataland.com/api\",\n",
    "    \"base_url_documents\": \"https://dataland.com/documents\",\n",
    "    \n",
    "    # API Token aus Environment\n",
    "    \"api_token\": os.getenv(\"DATALAND_TOKEN\"),\n",
    "    \n",
    "    # Unternehmen zum Testen\n",
    "    \"company_query\": \"Siemens\",\n",
    "    \n",
    "    # Company-Lookup Kandidaten (API-Service)\n",
    "    # Probiere verschiedene Endpunkte, da √∂ffentliche Swagger nicht eindeutig ist\n",
    "    \"company_lookup_api_candidates\": [\n",
    "        # {\"path\": \"/metadata/companies/search\", \"params\": {\"q\": None}},\n",
    "        # {\"path\": \"/metadata/companies\", \"params\": {\"query\": None}},\n",
    "        # {\"path\": \"/companies/search\", \"params\": {\"q\": None}},\n",
    "        # {\"path\": \"/companies\", \"params\": {\"name\": None}},\n",
    "        # {\"path\": \"/entities/search\", \"params\": {\"q\": None}},\n",
    "        # {\"path\": \"/api/companies/search\", \"params\": {\"q\": None}},\n",
    "        {\"path\": \"/companies/names\", \"params\": {\"searchString\": None}},\n",
    "    ],\n",
    "    \n",
    "    # Company-Lookup Kandidaten (Documents-Service) - Fallback\n",
    "    \"company_lookup_doc_candidates\": [\n",
    "        {\"path\": \"/\", \"params\": {\"q\": None}},\n",
    "        {\"path\": \"/search\", \"params\": {\"q\": None}},\n",
    "        {\"path\": \"/\", \"params\": {\"query\": None}},\n",
    "        {\"path\": \"/search\", \"params\": {\"query\": None}},\n",
    "        {\"path\": \"/companies/search\", \"params\": {\"q\": None}},\n",
    "        {\"path\": \"/documents/search\", \"params\": {\"q\": None}},\n",
    "    ],\n",
    "    \n",
    "    # Timeouts\n",
    "    \"timeout_search\": 30,\n",
    "    \"timeout_data\": 20,\n",
    "    \n",
    "    # Retry-Konfiguration\n",
    "    \"max_retries\": 3,\n",
    "    \"backoff_base\": 2,        # Exponential Backoff Basis (2^retry_count)\n",
    "    \"backoff_max\": 60,        # Maximale Wartezeit in Sekunden\n",
    "    \"rate_limit_wait\": 60,    # Wartezeit bei Rate Limit (429)\n",
    "    \n",
    "    # Raw Data Directory\n",
    "    \"raw_dir\": Path(\"raw\")\n",
    "}\n",
    "\n",
    "# Erstelle raw/ Verzeichnis\n",
    "CONFIG[\"raw_dir\"].mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Konfiguration geladen:\")\n",
    "print(f\"   API Service: {CONFIG['base_url_api']}\")\n",
    "print(f\"   Documents Service: {CONFIG['base_url_documents']}\")\n",
    "print(f\"   Company Query: {CONFIG['company_query']}\")\n",
    "print(f\"   API-Lookup Kandidaten: {len(CONFIG['company_lookup_api_candidates'])}\")\n",
    "print(f\"   Docs-Lookup Kandidaten: {len(CONFIG['company_lookup_doc_candidates'])}\")\n",
    "print(f\"   Token verf√ºgbar: {'‚úÖ Ja' if CONFIG['api_token'] else '‚ùå Nein (.env fehlt)'}\")\n",
    "print(f\"   Retry-Config: max={CONFIG['max_retries']}, backoff={CONFIG['backoff_base']}^n (max {CONFIG['backoff_max']}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1e63f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Helper-Funktionen f√ºr die API-Kommunikation\n",
    "\n",
    "Bevor wir Daten abrufen, erstellen wir wiederverwendbare Funktionen.\n",
    "\n",
    "### Was macht ein guter Connector?\n",
    "\n",
    "Ein professioneller Connector sollte:\n",
    "- **Fehler behandeln**: Was passiert, wenn die API nicht antwortet?\n",
    "- **Authentifizierung**: Korrektes Senden von API-Keys\n",
    "- **Logging**: Was wird gerade gemacht? Gab es Probleme?\n",
    "- **Retry-Logik**: Bei tempor√§ren Fehlern automatisch nochmal versuchen\n",
    "- **Pagination**: Gro√üe Datenmengen in Teilen abrufen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a5cae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. JSONL Raw-Persistenz Funktionen\n",
    "\n",
    "Bevor wir Daten von der API abrufen, erstellen wir Funktionen zum Speichern der Rohdaten.\n",
    "\n",
    "### Was ist Raw-Persistenz?\n",
    "\n",
    "**Raw-Persistenz** bedeutet, dass wir die Daten **exakt so** speichern, wie wir sie von der API bekommen:\n",
    "- ‚úÖ Keine Transformation\n",
    "- ‚úÖ Keine Normalisierung  \n",
    "- ‚úÖ Komplette API-Response\n",
    "- ‚úÖ Metadaten zum Request (Zeitstempel, Endpoint, Status)\n",
    "\n",
    "### Warum JSONL?\n",
    "\n",
    "**JSONL** (JSON Lines) ist perfekt f√ºr unseren Use-Case:\n",
    "- Eine Zeile = ein API-Response\n",
    "- Einfach zu erweitern (append-only)\n",
    "- Sp√§ter leicht zu verarbeiten\n",
    "- Fehlertoleranz (eine kaputte Zeile ‚â† kaputte Datei)\n",
    "\n",
    "### Envelope-Format\n",
    "\n",
    "Jede Zeile enth√§lt ein \"Envelope\" mit Metadaten:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"ts\": \"2025-11-05T13:45:12Z\",\n",
    "  \"endpoint\": \"/api/metadata/companies/search\",\n",
    "  \"status\": 200,\n",
    "  \"request\": {\"params\": {\"q\": \"Siemens\"}},\n",
    "  \"payload\": { /* Original API-Response */ }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "834a0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSONL-Persistenz-Funktionen definiert\n",
      "   - nowz(): 2025-11-10T10:02:00Z\n",
      "   - append_jsonl(): Bereit zum Schreiben\n",
      "   - create_envelope(): Bereit f√ºr API-Responses\n",
      "   - generate_hash(): Bereit f√ºr Idempotenz-Checks\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def nowz() -> str:\n",
    "    \"\"\"\n",
    "    Gibt aktuellen UTC-Zeitstempel im ISO-Format zur√ºck.\n",
    "    \n",
    "    Returns:\n",
    "        String wie \"2025-11-05T13:45:12Z\"\n",
    "    \"\"\"\n",
    "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    F√ºgt ein JSON-Objekt als neue Zeile zu einer JSONL-Datei hinzu.\n",
    "    \n",
    "    Args:\n",
    "        path: Pfad zur JSONL-Datei\n",
    "        obj: Dictionary, das gespeichert werden soll\n",
    "    \"\"\"\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def create_envelope(endpoint: str, status: int, request_params: Dict[str, Any], \n",
    "                    payload: Any, error: Optional[Any] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Envelope mit Metadaten + optionalem Fehlerfeld erstellen.\n",
    "    - endpoint: aufgerufener API-Pfad (ohne Base-URL)\n",
    "    - status: HTTP-Statuscode (oder -1 bei Ausnahme)\n",
    "    - request_params: Query-Parameter\n",
    "    - payload: Response-Body (roh)\n",
    "    - error: optionaler Fehlertext/-objekt\n",
    "    \"\"\"\n",
    "    env = {\n",
    "        \"ts\": nowz(),\n",
    "        \"endpoint\": endpoint,\n",
    "        \"status\": status,\n",
    "        \"request\": {\"params\": request_params},\n",
    "        \"payload\": payload\n",
    "    }\n",
    "    if error is not None:\n",
    "        env[\"error\"] = error\n",
    "    return env\n",
    "\n",
    "\n",
    "def generate_hash(company_id: str, data_point_id: str = None, \n",
    "                  period: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generiert einen SHA256-Hash f√ºr Idempotenz-Checks.\n",
    "    \n",
    "    Verhindert, dass dieselben Daten mehrfach gespeichert werden.\n",
    "    \n",
    "    Args:\n",
    "        company_id: Die Unternehmens-ID\n",
    "        data_point_id: Optional - Datenpunkt-ID\n",
    "        period: Optional - Periode\n",
    "        \n",
    "    Returns:\n",
    "        Hexadezimaler Hash-String\n",
    "    \"\"\"\n",
    "    key = f\"{company_id}:{data_point_id or ''}:{period or ''}\"\n",
    "    return hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "\n",
    "print(\"‚úÖ JSONL-Persistenz-Funktionen definiert\")\n",
    "print(f\"   - nowz(): {nowz()}\")\n",
    "print(f\"   - append_jsonl(): Bereit zum Schreiben\")\n",
    "print(f\"   - create_envelope(): Bereit f√ºr API-Responses\")\n",
    "print(f\"   - generate_hash(): Bereit f√ºr Idempotenz-Checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d9d6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. HTTP-Session mit Retry-Logik & Authentifizierung\n",
    "\n",
    "Jetzt erstellen wir eine robuste HTTP-Session f√ºr die API-Kommunikation.\n",
    "\n",
    "### Was macht diese Session besonders?\n",
    "\n",
    "1. **Authentifizierung**: Bearer-Token automatisch in jedem Request\n",
    "2. **Retry-Logik**: Bei tempor√§ren Fehlern automatisch wiederholen\n",
    "3. **Exponential Backoff**: Bei Rate Limits (HTTP 429) intelligente Wartezeiten\n",
    "4. **Fehlerbehandlung**: Unterscheidung zwischen permanenten und tempor√§ren Fehlern\n",
    "5. **Logging**: Detaillierte Ausgaben f√ºr Debugging\n",
    "\n",
    "### Welche Fehler werden behandelt?\n",
    "\n",
    "- **HTTP 429** (Too Many Requests): Rate Limit erreicht ‚Üí Exponential Backoff\n",
    "- **HTTP 5xx** (Server-Fehler): Tempor√§rer Fehler ‚Üí Retry\n",
    "- **Timeout**: Netzwerk-Problem ‚Üí Retry\n",
    "- **HTTP 4xx** (au√üer 429): Client-Fehler ‚Üí Kein Retry (z.B. 404, 401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4aaeba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß ERSTELLE ZWEI GETRENNTE SESSIONS\n",
      "======================================================================\n",
      "‚úÖ Session f√ºr https://dataland.com/api mit Token Bear...7586 initialisiert\n",
      "‚úÖ Session f√ºr https://dataland.com/documents mit Token Bear...7586 initialisiert\n",
      "\n",
      "‚úÖ Beide Sessions bereit f√ºr API-Calls!\n"
     ]
    }
   ],
   "source": [
    "class DatalandHTTPSession:\n",
    "    \"\"\"\n",
    "    HTTP-Session mit Retry-Logik, Exponential Backoff und Authentifizierung.\n",
    "    \n",
    "    Diese Klasse kapselt alle HTTP-Kommunikation mit der Dataland API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_token: Optional[str], config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialisiert die HTTP-Session.\n",
    "        \n",
    "        Args:\n",
    "            base_url: Die Base-URL f√ºr diese Session (z.B. /api oder /documents)\n",
    "            api_token: Bearer-Token f√ºr Authentifizierung\n",
    "            config: Konfigurationsdictionary mit Timeouts, Retries, etc.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = config[\"max_retries\"]\n",
    "        self.backoff_base = config[\"backoff_base\"]\n",
    "        self.backoff_max = config[\"backoff_max\"]\n",
    "        \n",
    "        # Session erstellen\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Headers setzen\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Bearer-Token hinzuf√ºgen (falls vorhanden)\n",
    "        if api_token:\n",
    "            # print(api_token)\n",
    "            headers[\"Authorization\"] = f\"{api_token}\"\n",
    "            # Zeige nur erste/letzte 4 Zeichen f√ºr Debugging\n",
    "            token_preview = f\"{api_token[:4]}...{api_token[-4:]}\" if len(api_token) > 8 else \"***\"\n",
    "            print(f\"‚úÖ Session f√ºr {base_url} mit Token {token_preview} initialisiert\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Session f√ºr {base_url} ohne Token (Mock-Modus)\")\n",
    "        \n",
    "        self.session.headers.update(headers)\n",
    "        \n",
    "        # Statistiken\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"retries\": 0,\n",
    "            \"rate_limits\": 0\n",
    "        }\n",
    "    \n",
    "    def _calculate_backoff(self, attempt: int) -> float:\n",
    "        \"\"\"\n",
    "        Berechnet Wartezeit f√ºr Exponential Backoff.\n",
    "        \n",
    "        Args:\n",
    "            attempt: Versuch-Nummer (1-basiert)\n",
    "            \n",
    "        Returns:\n",
    "            Wartezeit in Sekunden\n",
    "        \"\"\"\n",
    "        wait = min(self.backoff_base ** attempt, self.backoff_max)\n",
    "        return wait\n",
    "    \n",
    "    def get(self, endpoint: str, params: Optional[Dict] = None, \n",
    "            timeout: int = 30) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        F√ºhrt GET-Request mit Retry-Logik aus.\n",
    "        \n",
    "        Args:\n",
    "            endpoint: API-Endpoint (z.B. \"/\" oder \"/search\")\n",
    "            params: Query-Parameter\n",
    "            timeout: Timeout in Sekunden\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mit {\"status\": int, \"data\": Any, \"error\": str|None}\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        params = params or {}\n",
    "        \n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                print(f\"üîÑ GET {endpoint} (Versuch {attempt}/{self.max_retries})\")\n",
    "                print(f\"   URL: {url}\")\n",
    "                if params:\n",
    "                    print(f\"   Params: {params}\")\n",
    "                \n",
    "                response = self.session.get(url, params=params, timeout=timeout)\n",
    "                \n",
    "                # Erfolg!\n",
    "                if response.status_code == 200:\n",
    "                    self.stats[\"successful_requests\"] += 1\n",
    "                    print(f\"‚úÖ Status 200 - Erfolg\")\n",
    "                    return {\n",
    "                        \"status\": 200,\n",
    "                        \"data\": response.json(),\n",
    "                        \"error\": None\n",
    "                    }\n",
    "                \n",
    "                # Rate Limit (429)\n",
    "                elif response.status_code == 429:\n",
    "                    self.stats[\"rate_limits\"] += 1\n",
    "                    wait = self._calculate_backoff(attempt)\n",
    "                    print(f\"‚è±Ô∏è  Rate Limit (429) - Warte {wait}s...\")\n",
    "                    time.sleep(wait)\n",
    "                    self.stats[\"retries\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Server-Fehler (5xx) - Retry\n",
    "                elif 500 <= response.status_code < 600:\n",
    "                    print(f\"‚ö†Ô∏è  Server-Fehler ({response.status_code})\")\n",
    "                    if attempt < self.max_retries:\n",
    "                        wait = self._calculate_backoff(attempt)\n",
    "                        print(f\"   Retry in {wait}s...\")\n",
    "                        time.sleep(wait)\n",
    "                        self.stats[\"retries\"] += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.stats[\"failed_requests\"] += 1\n",
    "                        return {\n",
    "                            \"status\": response.status_code,\n",
    "                            \"data\": None,\n",
    "                            \"error\": f\"Server-Fehler nach {self.max_retries} Versuchen\"\n",
    "                        }\n",
    "                \n",
    "                # Client-Fehler (4xx) - Kein Retry\n",
    "                else:\n",
    "                    self.stats[\"failed_requests\"] += 1\n",
    "                    print(f\"‚ùå Client-Fehler ({response.status_code})\")\n",
    "                    try:\n",
    "                        error_data = response.json()\n",
    "                    except:\n",
    "                        error_data = response.text\n",
    "                    return {\n",
    "                        \"status\": response.status_code,\n",
    "                        \"data\": None,\n",
    "                        \"error\": f\"HTTP {response.status_code}: {error_data}\"\n",
    "                    }\n",
    "            \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è±Ô∏è  Timeout bei Versuch {attempt}\")\n",
    "                if attempt < self.max_retries:\n",
    "                    wait = self._calculate_backoff(attempt)\n",
    "                    print(f\"   Retry in {wait}s...\")\n",
    "                    time.sleep(wait)\n",
    "                    self.stats[\"retries\"] += 1\n",
    "                else:\n",
    "                    self.stats[\"failed_requests\"] += 1\n",
    "                    return {\n",
    "                        \"status\": 0,\n",
    "                        \"data\": None,\n",
    "                        \"error\": f\"Timeout nach {self.max_retries} Versuchen\"\n",
    "                    }\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"‚ùå Netzwerk-Fehler: {e}\")\n",
    "                if attempt < self.max_retries:\n",
    "                    wait = self._calculate_backoff(attempt)\n",
    "                    print(f\"   Retry in {wait}s...\")\n",
    "                    time.sleep(wait)\n",
    "                    self.stats[\"retries\"] += 1\n",
    "                else:\n",
    "                    self.stats[\"failed_requests\"] += 1\n",
    "                    return {\n",
    "                        \"status\": 0,\n",
    "                        \"data\": None,\n",
    "                        \"error\": f\"Netzwerk-Fehler: {str(e)}\"\n",
    "                    }\n",
    "        \n",
    "        # Sollte nie erreicht werden\n",
    "        self.stats[\"failed_requests\"] += 1\n",
    "        return {\n",
    "            \"status\": 0,\n",
    "            \"data\": None,\n",
    "            \"error\": \"Maximale Retries erreicht\"\n",
    "        }\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Gibt Statistiken aus.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìä HTTP-SESSION STATISTIKEN ({self.base_url})\")\n",
    "        print(\"=\"*60)\n",
    "        for key, value in self.stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ‚ö†Ô∏è WICHTIG: ZWEI Sessions erstellen - eine f√ºr jeden Service!\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîß ERSTELLE ZWEI GETRENNTE SESSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Session 1: API Service (Metadaten, Dimensions, Values)\n",
    "api_session = DatalandHTTPSession(\n",
    "    base_url=CONFIG[\"base_url_api\"],\n",
    "    api_token=CONFIG[\"api_token\"],\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "# Session 2: Documents Service (Reports, Sustainability Documents)\n",
    "doc_session = DatalandHTTPSession(\n",
    "    base_url=CONFIG[\"base_url_documents\"],\n",
    "    api_token=CONFIG[\"api_token\"],\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Beide Sessions bereit f√ºr API-Calls!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce75b92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç SCHRITT 1a: COMPANY-LOOKUP VIA API SERVICE (Prim√§rweg)\n",
    "\n",
    "**Ziel:** Finde die `company_id` √ºber den API-Service (nicht Documents).\n",
    "\n",
    "**Problem:** Die Swagger-Dokumentation zeigt keinen eindeutigen `/metadata/companies/search` Endpoint. Daher implementieren wir einen **systematischen Probing-Ansatz**:\n",
    "\n",
    "### Strategie:\n",
    "\n",
    "1. **Probe-Liste:** Teste verschiedene Endpoint-Kandidaten aus der Konfiguration\n",
    "2. **Persistenz:** Speichere alle Versuche in `raw/company_lookup_api.jsonl`\n",
    "3. **Fallback:** Wenn API-Lookup fehlschl√§gt ‚Üí Documents-Service nutzen\n",
    "\n",
    "### Warum API-Service bevorzugen?\n",
    "\n",
    "- Direkte `company_id` ohne Umweg √ºber Documents\n",
    "- Konsistent mit `/metadata/available-data-dimensions` Flow\n",
    "- Bessere Performance (keine Document-Metadaten laden)\n",
    "\n",
    "### Typische Response:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"companies\": [\n",
    "    {\n",
    "      \"companyId\": \"dataland_siemens_ag_001\",\n",
    "      \"name\": \"Siemens AG\",\n",
    "      \"sector\": \"Industrials\",\n",
    "      \"country\": \"DE\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54383191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç SCHRITT 1a: COMPANY-LOOKUP VIA API SERVICE (Prim√§rweg)\n",
      "======================================================================\n",
      "Suche nach: 'Siemens' √ºber API-Service\n",
      "Probe 1 Endpoint-Kandidaten...\n",
      "\n",
      "üß™ Probe 1/1: /companies/names\n",
      "   Params: {'searchString': 'Siemens'}\n",
      "üîÑ GET /companies/names (Versuch 1/3)\n",
      "   URL: https://dataland.com/api/companies/names\n",
      "   Params: {'searchString': 'Siemens'}\n",
      "‚úÖ Status 200 - Erfolg\n",
      "   ‚úÖ HTTP 200 - Daten erhalten!\n",
      "\n",
      "‚úÖ 100 Unternehmen gefunden:\n",
      "   1. Siemens Aktiengesellschaft (ID: f16a12ff-714c-4dd1-b141-eb8b0355c833, Sektor: N/A)\n",
      "   2. Siemens Energy AG (ID: 39260625-315b-4158-97aa-64b69eb331db, Sektor: N/A)\n",
      "   3. SIEMENS FINANCIERINGSMAATSCHAPPIJ N.V. (ID: adee0944-7d21-46cf-977a-6ec988b782a1, Sektor: N/A)\n",
      "   4. Siemens Healthineers AG (ID: cb57b6f0-8523-4c74-9dde-ff20e727a55c, Sektor: N/A)\n",
      "   5. Siemens Proprietary Limited (ID: 2be341ba-1336-4113-aa51-59f4a742726f, Sektor: N/A)\n",
      "   ... und 95 weitere\n",
      "\n",
      "‚ö†Ô∏è  Mehrere Treffer - nutze ersten: Siemens Aktiengesellschaft\n",
      "\n",
      "üìå Ausgew√§hltes Unternehmen (via API):\n",
      "   Name: Siemens Aktiengesellschaft\n",
      "   ID: f16a12ff-714c-4dd1-b141-eb8b0355c833\n",
      "   Sektor: None\n",
      "   Land: None\n",
      "\n",
      "======================================================================\n",
      "‚úÖ COMPANY-LOOKUP ERFOLGREICH\n",
      "======================================================================\n",
      "   Methode: API\n",
      "   Company ID: f16a12ff-714c-4dd1-b141-eb8b0355c833\n",
      "   Name: Siemens Aktiengesellschaft\n",
      "\n",
      "üîó Finale Company ID (Variable): f16a12ff-714c-4dd1-b141-eb8b0355c833\n"
     ]
    }
   ],
   "source": [
    "def search_company_via_api(api_session: DatalandHTTPSession, query: str, \n",
    "                           raw_dir: Path) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Sucht nach einem Unternehmen √ºber API-Service (Prim√§rweg).\n",
    "    \n",
    "    ‚ö†Ô∏è WICHTIG: Nutzt api_session, nicht doc_session!\n",
    "    \n",
    "    Probiert systematisch verschiedene Endpoint-Kandidaten aus der Config,\n",
    "    da √∂ffentliche Swagger keinen eindeutigen Company-Search dokumentiert.\n",
    "    \n",
    "    Args:\n",
    "        api_session: DatalandHTTPSession f√ºr API-Service\n",
    "        query: Suchbegriff (z.B. \"Siemens\")\n",
    "        raw_dir: Verzeichnis f√ºr Raw-Daten\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mit company_id, name, sector, country oder None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç SCHRITT 1a: COMPANY-LOOKUP VIA API SERVICE (Prim√§rweg)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Suche nach: '{query}' √ºber API-Service\")\n",
    "    print(f\"Probe {len(CONFIG['company_lookup_api_candidates'])} Endpoint-Kandidaten...\")\n",
    "    \n",
    "    candidates = CONFIG[\"company_lookup_api_candidates\"]\n",
    "    successful_result = None\n",
    "    \n",
    "    for idx, candidate in enumerate(candidates, 1):\n",
    "        endpoint = candidate[\"path\"]\n",
    "        param_key = list(candidate[\"params\"].keys())[0]\n",
    "        params = {param_key: query}\n",
    "        \n",
    "        print(f\"\\nüß™ Probe {idx}/{len(candidates)}: {endpoint}\")\n",
    "        print(f\"   Params: {params}\")\n",
    "        \n",
    "        result = api_session.get(endpoint, params=params, timeout=CONFIG[\"timeout_search\"])\n",
    "        \n",
    "        # Raw-Persistenz\n",
    "        envelope = create_envelope(endpoint, result[\"status\"], params, result[\"data\"], \n",
    "                                   error=result.get(\"error\"))\n",
    "        append_jsonl(raw_dir / \"company_lookup_api.jsonl\", envelope)\n",
    "        \n",
    "        if result[\"status\"] == 200 and result[\"data\"]:\n",
    "            print(f\"   ‚úÖ HTTP 200 - Daten erhalten!\")\n",
    "            successful_result = result\n",
    "            break\n",
    "        elif result[\"status\"] == 404:\n",
    "            print(f\"   ‚ùå HTTP 404 - Endpoint existiert nicht\")\n",
    "        elif result[\"status\"] == 401:\n",
    "            print(f\"   ‚ùå HTTP 401 - Auth-Problem\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå HTTP {result['status']}\")\n",
    "    \n",
    "    if not successful_result:\n",
    "        print(f\"\\n‚ùå Alle {len(candidates)} API-Probes fehlgeschlagen\")\n",
    "        print(f\"üíæ Alle Versuche gespeichert in: company_lookup_api.jsonl\")\n",
    "        print(f\"üîÑ Fallback: Wechsle zu Documents-Service...\")\n",
    "        return None\n",
    "    \n",
    "    # Analysiere Response\n",
    "    response_data = successful_result[\"data\"]\n",
    "    \n",
    "    # Finde Company-Array (verschiedene Feldnamen m√∂glich)\n",
    "    companies = None\n",
    "    for possible_key in [\"companies\", \"results\", \"data\", \"items\", \"entities\"]:\n",
    "        if possible_key in response_data:\n",
    "            companies = response_data[possible_key]\n",
    "            break\n",
    "    \n",
    "    # Falls direkt ein Array zur√ºckkommt\n",
    "    if companies is None and isinstance(response_data, list):\n",
    "        companies = response_data\n",
    "    \n",
    "    # Falls einzelnes Objekt (kein Array)\n",
    "    if companies is None and isinstance(response_data, dict):\n",
    "        companies = [response_data]\n",
    "    \n",
    "    if not companies:\n",
    "        print(f\"‚ö†Ô∏è  Keine Unternehmen gefunden (leere Response)\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(companies)} Unternehmen gefunden:\")\n",
    "    \n",
    "    # Zeige ersten 5\n",
    "    for i, comp in enumerate(companies[:5], 1):\n",
    "        name = comp.get(\"name\") or comp.get(\"companyName\") or comp.get(\"company\") or \"?\"\n",
    "        cid = comp.get(\"companyId\") or comp.get(\"id\") or comp.get(\"dataId\") or \"?\"\n",
    "        sector = comp.get(\"sector\") or \"N/A\"\n",
    "        print(f\"   {i}. {name} (ID: {cid}, Sektor: {sector})\")\n",
    "    \n",
    "    if len(companies) > 5:\n",
    "        print(f\"   ... und {len(companies) - 5} weitere\")\n",
    "    \n",
    "    # Nimm das erste Ergebnis (bei mehreren Treffern)\n",
    "    selected = companies[0]\n",
    "    \n",
    "    if len(companies) > 1:\n",
    "        print(f\"\\n‚ö†Ô∏è  Mehrere Treffer - nutze ersten: {selected.get('name') or selected.get('companyName')}\")\n",
    "    \n",
    "    # Extrahiere relevante Felder (flexibel)\n",
    "    company_info = {\n",
    "        \"company_id\": selected.get(\"companyId\") or selected.get(\"id\") or selected.get(\"dataId\"),\n",
    "        \"name\": selected.get(\"companyName\") or selected.get(\"name\") or selected.get(\"company\"),\n",
    "        \"sector\": selected.get(\"sector\"),\n",
    "        \"country\": selected.get(\"countryCode\") or selected.get(\"country\"),\n",
    "        \"source\": \"api\",  # Markiere als API-Lookup\n",
    "        \"raw\": selected\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìå Ausgew√§hltes Unternehmen (via API):\")\n",
    "    print(f\"   Name: {company_info['name']}\")\n",
    "    print(f\"   ID: {company_info['company_id']}\")\n",
    "    print(f\"   Sektor: {company_info.get('sector', 'N/A')}\")\n",
    "    print(f\"   Land: {company_info.get('country', 'N/A')}\")\n",
    "    \n",
    "    return company_info\n",
    "\n",
    "\n",
    "def search_company_via_documents(doc_session: DatalandHTTPSession, query: str, \n",
    "                                  raw_dir: Path) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Sucht nach einem Unternehmen √ºber Documents-Service (Fallback).\n",
    "    \n",
    "    ‚ö†Ô∏è WICHTIG: Nutzt doc_session, nicht api_session!\n",
    "    \n",
    "    Args:\n",
    "        doc_session: DatalandHTTPSession f√ºr Documents-Service\n",
    "        query: Suchbegriff (z.B. \"Siemens\")\n",
    "        raw_dir: Verzeichnis f√ºr Raw-Daten\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mit company_id, name, sector, country oder None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç SCHRITT 1b: COMPANY-LOOKUP VIA DOCUMENTS SERVICE (Fallback)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Suche nach: '{query}' √ºber Documents-Service\")\n",
    "    \n",
    "    candidates = CONFIG[\"company_lookup_doc_candidates\"]\n",
    "    successful_result = None\n",
    "    \n",
    "    for idx, candidate in enumerate(candidates, 1):\n",
    "        endpoint = candidate[\"path\"]\n",
    "        param_key = list(candidate[\"params\"].keys())[0]\n",
    "        params = {param_key: query}\n",
    "        \n",
    "        print(f\"\\nüß™ Probe {idx}/{len(candidates)}: {endpoint}\")\n",
    "        print(f\"   Params: {params}\")\n",
    "        \n",
    "        result = doc_session.get(endpoint, params=params, timeout=CONFIG[\"timeout_search\"])\n",
    "        \n",
    "        # Raw-Persistenz\n",
    "        envelope = create_envelope(endpoint, result[\"status\"], params, result[\"data\"],\n",
    "                                   error=result.get(\"error\"))\n",
    "        append_jsonl(raw_dir / \"company_lookup_documents.jsonl\", envelope)\n",
    "        \n",
    "        if result[\"status\"] == 200 and result[\"data\"]:\n",
    "            print(f\"   ‚úÖ HTTP 200 - Daten erhalten!\")\n",
    "            successful_result = result\n",
    "            break\n",
    "        elif result[\"status\"] == 404:\n",
    "            print(f\"   ‚ùå HTTP 404 - Endpoint existiert nicht\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå HTTP {result['status']}\")\n",
    "    \n",
    "    if not successful_result:\n",
    "        print(f\"\\n‚ùå Alle {len(candidates)} Documents-Probes fehlgeschlagen\")\n",
    "        print(f\"üíæ Alle Versuche gespeichert in: company_lookup_documents.jsonl\")\n",
    "        return None\n",
    "    \n",
    "    # Analysiere Response (gleiche Logik wie API)\n",
    "    response_data = successful_result[\"data\"]\n",
    "    \n",
    "    companies = None\n",
    "    for possible_key in [\"companies\", \"results\", \"data\", \"items\", \"documents\"]:\n",
    "        if possible_key in response_data:\n",
    "            companies = response_data[possible_key]\n",
    "            break\n",
    "    \n",
    "    if companies is None and isinstance(response_data, list):\n",
    "        companies = response_data\n",
    "    \n",
    "    if companies is None and isinstance(response_data, dict):\n",
    "        companies = [response_data]\n",
    "    \n",
    "    if not companies:\n",
    "        print(f\"‚ö†Ô∏è  Keine Unternehmen gefunden\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(companies)} Unternehmen gefunden (via Documents):\")\n",
    "    \n",
    "    for i, comp in enumerate(companies[:5], 1):\n",
    "        name = comp.get(\"name\") or comp.get(\"companyName\") or \"?\"\n",
    "        cid = comp.get(\"companyId\") or comp.get(\"id\") or \"?\"\n",
    "        print(f\"   {i}. {name} (ID: {cid})\")\n",
    "    \n",
    "    selected = companies[0]\n",
    "    \n",
    "    company_info = {\n",
    "        \"company_id\": selected.get(\"companyId\") or selected.get(\"id\") or selected.get(\"dataId\"),\n",
    "        \"name\": selected.get(\"companyName\") or selected.get(\"name\"),\n",
    "        \"sector\": selected.get(\"sector\"),\n",
    "        \"country\": selected.get(\"countryCode\") or selected.get(\"country\"),\n",
    "        \"source\": \"documents\",  # Markiere als Documents-Lookup\n",
    "        \"raw\": selected\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìå Ausgew√§hltes Unternehmen (via Documents):\")\n",
    "    print(f\"   Name: {company_info['name']}\")\n",
    "    print(f\"   ID: {company_info['company_id']}\")\n",
    "    print(f\"   Source: Documents-Service (Fallback)\")\n",
    "    \n",
    "    return company_info\n",
    "\n",
    "\n",
    "# Company-Lookup mit Fallback-Strategie\n",
    "company_info = None\n",
    "\n",
    "# 1. Versuch: API-Service (Prim√§rweg)\n",
    "company_info = search_company_via_api(api_session, CONFIG[\"company_query\"], CONFIG[\"raw_dir\"])\n",
    "\n",
    "# 2. Versuch: Documents-Service (Fallback)\n",
    "if company_info is None:\n",
    "    print(\"\\n\" + \"‚ö†Ô∏è \"*35)\n",
    "    print(\"‚ö†Ô∏è  API-Lookup fehlgeschlagen - aktiviere Documents-Fallback\")\n",
    "    print(\"‚ö†Ô∏è \"*35)\n",
    "    company_info = search_company_via_documents(doc_session, CONFIG[\"company_query\"], CONFIG[\"raw_dir\"])\n",
    "\n",
    "# Finale Auswertung\n",
    "if company_info:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ COMPANY-LOOKUP ERFOLGREICH\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Methode: {company_info.get('source', 'unknown').upper()}\")\n",
    "    print(f\"   Company ID: {company_info['company_id']}\")\n",
    "    print(f\"   Name: {company_info['name']}\")\n",
    "\n",
    "    # üîΩüîΩüîΩ NEU: Finale Company-ID als Variable f√ºr den n√§chsten Schritt speichern\n",
    "    # ‚úÖ Finale Company-ID global speichern\n",
    "    global final_company_id\n",
    "    final_company_id = company_info[\"company_id\"]\n",
    "    print(f\"\\nüîó Finale Company ID (Variable): {final_company_id}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå COMPANY-LOOKUP KOMPLETT FEHLGESCHLAGEN\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  Weder API noch Documents lieferten Ergebnisse\")\n",
    "    print(\"‚ö†Ô∏è  Fahre mit Mock-Daten fort...\")\n",
    "    print(\"\\nüí° Tipp:\")\n",
    "    print(\"   - Pr√ºfe .env (DATALAND_TOKEN vorhanden?)\")\n",
    "    print(\"   - Pr√ºfe Netzwerk (Firewall, VPN?)\")\n",
    "    print(\"   - Pr√ºfe raw/*.jsonl Logs f√ºr Details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed6484",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä SCHRITT 2: VERF√úGBARE DIMENSIONEN ABRUFEN\n",
    "\n",
    "**Jetzt, wo wir eine `company_id` haben,** fragen wir die API: **Welche Daten sind f√ºr dieses Unternehmen verf√ºgbar?**\n",
    "\n",
    "### Was sind \"Dimensions\"?\n",
    "\n",
    "**Available Data Dimensions** (`/metadata/available-data-dimensions`) ist ein Metadaten-Endpoint, der zur√ºckgibt:\n",
    "- Welche **Datenpunkte** (Indicators) verf√ºgbar sind\n",
    "- F√ºr welche **Perioden** (Jahre, Quartale) Daten vorliegen\n",
    "- Welche **Dimensionen** (z.B. Scope 1/2/3 bei CO‚ÇÇ) existieren\n",
    "- **IDs**, die wir f√ºr den Datenabruf brauchen\n",
    "\n",
    "### Swagger-Dokumentation:\n",
    "\n",
    "Dieser Endpoint ist offiziell dokumentiert:\n",
    "- **URL:** `https://dataland.com/api/swagger-ui/index.html`\n",
    "- **Ressource:** `/metadata/available-data-dimensions`\n",
    "- **Parameter:** `companyId` (Pflicht)\n",
    "\n",
    "### Beispiel-Response:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"availableDataPoints\": [\n",
    "    {\n",
    "      \"dataPointId\": \"co2_scope1_2023\",\n",
    "      \"period\": \"2023\",\n",
    "      \"dimension\": \"Scope1\",\n",
    "      \"indicator\": \"CO2_Emissions\",\n",
    "      \"unit\": \"Mt\"\n",
    "    },\n",
    "    {\n",
    "      \"dataPointId\": \"energy_total_2023\",\n",
    "      \"period\": \"2023\",\n",
    "      \"indicator\": \"Energy_Consumption\",\n",
    "      \"unit\": \"TWh\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Warum wichtig?\n",
    "\n",
    "Ohne Dimensions wissen wir nicht, **welche Daten √ºberhaupt abrufbar sind**. Jede Dimension liefert eine `dataPointId`, die wir im n√§chsten Schritt f√ºr den Value-Abruf brauchen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Verwende finale Company ID: f16a12ff-714c-4dd1-b141-eb8b0355c833\n",
      "\n",
      "üìé Verwende finale Company ID f√ºr Dimensions: f16a12ff-714c-4dd1-b141-eb8b0355c833\n",
      "\n",
      "======================================================================\n",
      "SCHRITT 2: AVAILABLE DIMENSIONS (via API-Service)\n",
      "======================================================================\n",
      "Hole verf√ºgbare Daten f√ºr company_id: f16a12ff-714c-4dd1-b141-eb8b0355c833\n",
      "‚û°Ô∏è API-Request an /metadata/available-data-dimensions\n",
      "{\n",
      "  \"companyIds\": [\n",
      "    \"f16a12ff-714c-4dd1-b141-eb8b0355c833\"\n",
      "  ],\n",
      "  \"reportingPeriodFrom\": 2020,\n",
      "  \"reportingPeriodTo\": 2025\n",
      "}\n",
      "üîÑ GET /metadata/available-data-dimensions (Versuch 1/3)\n",
      "   URL: https://dataland.com/api/metadata/available-data-dimensions\n",
      "   Params: {'companyIds': ['f16a12ff-714c-4dd1-b141-eb8b0355c833'], 'reportingPeriodFrom': 2020, 'reportingPeriodTo': 2025}\n",
      "‚úÖ Status 200 - Erfolg\n",
      "Dimensions gespeichert in: available_dimensions.jsonl\n",
      "\n",
      "421 Dimensionen gefunden:\n",
      "   1. ? (?) - ID: ?\n",
      "   2. ? (?) - ID: ?\n",
      "   3. ? (?) - ID: ?\n",
      "   4. ? (?) - ID: ?\n",
      "   5. ? (?) - ID: ?\n",
      "   6. ? (?) - ID: ?\n",
      "   7. ? (?) - ID: ?\n",
      "   8. ? (?) - ID: ?\n",
      "   9. ? (?) - ID: ?\n",
      "   10. ? (?) - ID: ?\n",
      "   ... und 411 weitere\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DIMENSIONS ERFOLGREICH ABGERUFEN\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Direkt vor dem Aufruf sicherstellen, dass final_company_id existiert\n",
    "print(f\"üìå Verwende finale Company ID: {final_company_id}\")\n",
    "\n",
    "def list_available_dimensions(api_session: DatalandHTTPSession, company_id: str,\n",
    "                              raw_dir: Path) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Fragt verf√ºgbare Datendimensionen f√ºr ein Unternehmen ab.\n",
    "    Verwendet die von Dataland erwarteten Filter:\n",
    "      - companyIds: Liste mit genau einer ID\n",
    "      - reportingPeriodFrom / reportingPeriodTo: sinnvoller Zeitraum\n",
    "    Persistiert Raw-Response in raw/available_dimensions.jsonl.\n",
    "    \"\"\"\n",
    "\n",
    "# üí° √úberschreibe sicherheitshalber company_id mit der globalen final_company_id\n",
    "    company_id = globals().get(\"final_company_id\", company_id)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCHRITT 2: AVAILABLE DIMENSIONS (via API-Service)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Hole verf√ºgbare Daten f√ºr company_id: {company_id}\")\n",
    "\n",
    "    endpoint = \"/metadata/available-data-dimensions\"\n",
    "\n",
    "    # Zeitraumfilter: z.B. die letzten 5 vollen Jahre bis inkl. aktuelles Jahr\n",
    "    year_now = datetime.now().year\n",
    "    params = {\n",
    "        \"companyIds\": [company_id],          # <- wichtig: Plural + Array\n",
    "        \"reportingPeriodFrom\": year_now - 5, # anpassbar\n",
    "        \"reportingPeriodTo\": year_now        # anpassbar\n",
    "        # Optional (sp√§ter): categories, indicatorIds etc., wenn ihr weiter filtern wollt\n",
    "    }\n",
    "\n",
    "    print(f\"‚û°Ô∏è API-Request an {endpoint}\")\n",
    "    print(json.dumps(params, indent=2))\n",
    "\n",
    "    result = api_session.get(endpoint, params=params, timeout=CONFIG[\"timeout_data\"])\n",
    "\n",
    "    # Raw-Persistenz\n",
    "    envelope = create_envelope(endpoint, result[\"status\"], params, result.get(\"data\"), error=result.get(\"error\"))\n",
    "    append_jsonl(raw_dir / \"available_dimensions.jsonl\", envelope)\n",
    "    print(\"Dimensions gespeichert in: available_dimensions.jsonl\")\n",
    "\n",
    "    # Status pr√ºfen\n",
    "    if result[\"status\"] != 200:\n",
    "        print(f\"Fehler beim Abruf: HTTP {result['status']}\")\n",
    "        if result.get(\"error\"):\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "            # Typische Hilfe bei 400:\n",
    "            if result[\"status\"] == 400:\n",
    "                print(\"   Hinweis: Pr√ºfe, dass 'companyIds' (Plural) eine Liste ist \"\n",
    "                      \"und dass ein Zeitraum (reportingPeriodFrom/To) gesetzt ist.\")\n",
    "        return None\n",
    "\n",
    "    # Response extrahieren (flexibel je nach Schema)\n",
    "    data = result.get(\"data\")\n",
    "    dims = None\n",
    "    if isinstance(data, dict):\n",
    "        for key in (\"availableDataPoints\", \"dimensions\", \"dataPoints\", \"data\", \"results\", \"items\"):\n",
    "            if key in data and isinstance(data[key], list):\n",
    "                dims = data[key]\n",
    "                break\n",
    "    if dims is None and isinstance(data, list):\n",
    "        dims = data\n",
    "\n",
    "    if not dims:\n",
    "        print(\"Keine Dimensionen gefunden (leere Response)\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n{len(dims)} Dimensionen gefunden:\")\n",
    "    for i, d in enumerate(dims[:10], 1):\n",
    "        dp_id = d.get(\"dataPointId\") or d.get(\"id\") or \"?\"\n",
    "        indicator = d.get(\"indicator\") or d.get(\"metric\") or d.get(\"code\") or \"?\"\n",
    "        period = d.get(\"period\") or d.get(\"year\") or \"?\"\n",
    "        print(f\"   {i}. {indicator} ({period}) - ID: {dp_id}\")\n",
    "    if len(dims) > 10:\n",
    "        print(f\"   ... und {len(dims) - 10} weitere\")\n",
    "\n",
    "    return dims\n",
    "\n",
    "\n",
    "# # Dimensions abrufen (via api_session!)\n",
    "# # ‚ö†Ô∏è WICHTIG: Nutzt api_session, NICHT doc_session!\n",
    "# dimensions = None\n",
    "# # if company_info and company_info.get(\"company_id\"):\n",
    "# #     dimensions = list_available_dimensions(\n",
    "# #         api_session,  # <- API Service nutzen!\n",
    "# #         company_info[\"company_id\"],\n",
    "# #         CONFIG[\"raw_dir\"]\n",
    "# #     )\n",
    "\n",
    "# # Verwendet direkt die finale Company-ID Variable\n",
    "# if 'final_company_id' in locals() and final_company_id:\n",
    "#     dimensions = list_available_dimensions(\n",
    "#         api_session,          # <- API Service nutzen!\n",
    "#         final_company_id,     # <- hier wird die finale ID verwendet!\n",
    "#         CONFIG[\"raw_dir\"]\n",
    "#     )\n",
    "\n",
    "#     if dimensions:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"‚úÖ DIMENSIONS ERFOLGREICH ABGERUFEN\")\n",
    "#         print(\"=\"*70)\n",
    "#     else:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"‚ùå DIMENSIONS FEHLER ODER LEER\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(\"‚ö†Ô∏è  Fahre mit Mock-Daten fort...\")\n",
    "# else:\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"‚ö†Ô∏è  √úberspringe Dimensions (keine company_id)\")\n",
    "#     print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# AUSF√úHRUNG\n",
    "# ======================================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Company Lookup\n",
    "# company_info = search_company_via_api(api_session, CONFIG[\"company_query\"], CONFIG[\"raw_dir\"])\n",
    "\n",
    "# 2Ô∏è‚É£ Available Dimensions abrufen ‚Äì NUR mit final_company_id\n",
    "if \"final_company_id\" in globals() and final_company_id:\n",
    "    print(f\"\\nüìé Verwende finale Company ID f√ºr Dimensions: {final_company_id}\")\n",
    "    dimensions = list_available_dimensions(api_session, final_company_id, CONFIG[\"raw_dir\"])\n",
    "\n",
    "    if dimensions:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ DIMENSIONS ERFOLGREICH ABGERUFEN\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚ùå DIMENSIONS FEHLER ODER LEER\")\n",
    "        print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è Keine finale Company-ID gefunden ‚Äì √úberspringe Dimensions\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1d5ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Values ziehen: Alle Daten systematisch abrufen\n",
    "\n",
    "Jetzt kommt der Hauptteil: F√ºr **jeden** Datenpunkt aus den Dimensions rufen wir die tats√§chlichen **Werte** ab.\n",
    "\n",
    "### Strategie: \"Alles Verf√ºgbare\"\n",
    "\n",
    "Wir iterieren √ºber alle Dimensions-Eintr√§ge und rufen f√ºr jeden:\n",
    "- Den **Wert** (Value)\n",
    "- Die **Metadaten** (Einheit, Quelle, Qualit√§t)\n",
    "- Weitere **Kontexte** (Scope, Kategorie, etc.)\n",
    "\n",
    "### Idempotenz & Duplicate-Vermeidung\n",
    "\n",
    "Um zu verhindern, dass wir denselben Datenpunkt mehrfach ziehen:\n",
    "- Generieren wir einen **Hash** aus `(company_id, data_point_id, period)`\n",
    "- Pr√ºfen, ob dieser Hash schon in einer Set-Variable existiert\n",
    "- Speichern nur, wenn neu\n",
    "\n",
    "### Fehlertoleranz\n",
    "\n",
    "- Einzelne Fehler (z.B. 404 f√ºr einen Datenpunkt) stoppen nicht die gesamte Pipeline\n",
    "- Jeder Fehler wird geloggt\n",
    "- Am Ende: Zusammenfassung von Erfolgen und Fehlern\n",
    "\n",
    "### Progress-Tracking\n",
    "\n",
    "Bei vielen Datenpunkten (>100):\n",
    "- Zeigen wir alle 10% den Fortschritt an\n",
    "- Geben Zwischenstatistiken aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3be03d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üíé SCHRITT 3: VALUES ZIEHEN (via API-Service)\n",
      "======================================================================\n",
      "Ziehe Werte f√ºr 421 Datenpunkte...\n",
      "\n",
      "üîÑ Starte Datenabruf...\n",
      "‚ö†Ô∏è  [1/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [2/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [3/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [4/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [5/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [6/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [7/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [8/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [9/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [10/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [11/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [12/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [13/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [14/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [15/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [16/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [17/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [18/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [19/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [20/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [21/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [22/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [23/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [24/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [25/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [26/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [27/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [28/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [29/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [30/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [31/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [32/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [33/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [34/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [35/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [36/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [37/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [38/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [39/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [40/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [41/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [42/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [43/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [44/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [45/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [46/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [47/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [48/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [49/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [50/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [51/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [52/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [53/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [54/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [55/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [56/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [57/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [58/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [59/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [60/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [61/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [62/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [63/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [64/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [65/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [66/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [67/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [68/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [69/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [70/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [71/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [72/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [73/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [74/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [75/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [76/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [77/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [78/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [79/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [80/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [81/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [82/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [83/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [84/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [85/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [86/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [87/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [88/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [89/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [90/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [91/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [92/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [93/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [94/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [95/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [96/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [97/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [98/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [99/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [100/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [101/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [102/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [103/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [104/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [105/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [106/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [107/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [108/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [109/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [110/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [111/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [112/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [113/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [114/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [115/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [116/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [117/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [118/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [119/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [120/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [121/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [122/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [123/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [124/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [125/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [126/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [127/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [128/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [129/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [130/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [131/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [132/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [133/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [134/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [135/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [136/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [137/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [138/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [139/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [140/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [141/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [142/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [143/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [144/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [145/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [146/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [147/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [148/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [149/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [150/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [151/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [152/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [153/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [154/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [155/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [156/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [157/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [158/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [159/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [160/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [161/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [162/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [163/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [164/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [165/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [166/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [167/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [168/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [169/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [170/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [171/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [172/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [173/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [174/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [175/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [176/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [177/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [178/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [179/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [180/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [181/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [182/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [183/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [184/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [185/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [186/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [187/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [188/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [189/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [190/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [191/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [192/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [193/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [194/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [195/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [196/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [197/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [198/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [199/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [200/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [201/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [202/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [203/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [204/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [205/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [206/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [207/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [208/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [209/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [210/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [211/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [212/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [213/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [214/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [215/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [216/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [217/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [218/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [219/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [220/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [221/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [222/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [223/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [224/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [225/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [226/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [227/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [228/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [229/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [230/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [231/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [232/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [233/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [234/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [235/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [236/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [237/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [238/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [239/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [240/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [241/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [242/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [243/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [244/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [245/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [246/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [247/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [248/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [249/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [250/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [251/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [252/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [253/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [254/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [255/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [256/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [257/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [258/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [259/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [260/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [261/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [262/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [263/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [264/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [265/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [266/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [267/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [268/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [269/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [270/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [271/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [272/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [273/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [274/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [275/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [276/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [277/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [278/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [279/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [280/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [281/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [282/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [283/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [284/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [285/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [286/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [287/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [288/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [289/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [290/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [291/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [292/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [293/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [294/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [295/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [296/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [297/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [298/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [299/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [300/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [301/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [302/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [303/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [304/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [305/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [306/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [307/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [308/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [309/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [310/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [311/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [312/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [313/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [314/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [315/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [316/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [317/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [318/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [319/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [320/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [321/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [322/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [323/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [324/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [325/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [326/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [327/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [328/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [329/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [330/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [331/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [332/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [333/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [334/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [335/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [336/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [337/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [338/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [339/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [340/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [341/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [342/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [343/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [344/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [345/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [346/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [347/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [348/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [349/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [350/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [351/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [352/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [353/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [354/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [355/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [356/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [357/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [358/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [359/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [360/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [361/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [362/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [363/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [364/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [365/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [366/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [367/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [368/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [369/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [370/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [371/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [372/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [373/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [374/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [375/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [376/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [377/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [378/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [379/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [380/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [381/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [382/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [383/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [384/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [385/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [386/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [387/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [388/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [389/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [390/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [391/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [392/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [393/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [394/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [395/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [396/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [397/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [398/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [399/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [400/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [401/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [402/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [403/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [404/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [405/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [406/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [407/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [408/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [409/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [410/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [411/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [412/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [413/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [414/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [415/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [416/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [417/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [418/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [419/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [420/421] √úberspringe: Keine data_point_id\n",
      "‚ö†Ô∏è  [421/421] √úberspringe: Keine data_point_id\n",
      "\n",
      "======================================================================\n",
      "üìä VALUES-ABRUF ABGESCHLOSSEN\n",
      "======================================================================\n",
      "üíæ Daten gespeichert in: values_20251110T173624.jsonl\n",
      "\n",
      "üìà Statistiken:\n",
      "   Total: 421\n",
      "   Erfolgreich: 0\n",
      "   Fehlgeschlagen: 421\n",
      "   Duplikate √ºbersprungen: 0\n",
      "   Dauer: 0.0s\n",
      "   Rate: 95712.6 Datenpunkte/s\n",
      "\n",
      "‚ùå KEINE VALUES ABGERUFEN\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_values(api_session: DatalandHTTPSession, company_id: str,\n",
    "                     dimensions: List[Dict[str, Any]], raw_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ruft alle Werte f√ºr die gegebenen Dimensions ab.\n",
    "    \n",
    "    ‚ö†Ô∏è WICHTIG: Nutzt api_session (API Service), nicht doc_session!\n",
    "    \n",
    "    Speichert jede Response in raw/values_<timestamp>.jsonl\n",
    "    \n",
    "    Args:\n",
    "        api_session: DatalandHTTPSession f√ºr API-Service\n",
    "        company_id: Die Unternehmens-ID\n",
    "        dimensions: Liste der Dimensions (aus list_available_dimensions)\n",
    "        raw_dir: Verzeichnis f√ºr Raw-Daten\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mit Statistiken\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üíé SCHRITT 3: VALUES ZIEHEN (via API-Service)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Ziehe Werte f√ºr {len(dimensions)} Datenpunkte...\")\n",
    "    \n",
    "    # Zeitstempel f√ºr Dateiname\n",
    "    timestamp = nowz().replace(\":\", \"\").replace(\"-\", \"\").replace(\"Z\", \"\")\n",
    "    values_file = raw_dir / f\"values_{timestamp}.jsonl\"\n",
    "    \n",
    "    # Statistiken\n",
    "    stats = {\n",
    "        \"total\": len(dimensions),\n",
    "        \"success\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"skipped_duplicates\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    # Duplicate-Tracking\n",
    "    seen_hashes = set()\n",
    "    \n",
    "    # API-Endpoint (aus Swagger √ºbernehmen!)\n",
    "    endpoint = \"/data-points/values\"\n",
    "    \n",
    "    # Fortschrittsbalken-Schritte (alle 10%)\n",
    "    progress_step = max(1, len(dimensions) // 10)\n",
    "    \n",
    "    print(f\"\\nüîÑ Starte Datenabruf...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, dim in enumerate(dimensions, 1):\n",
    "        # Extrahiere IDs (Feldnamen aus Swagger anpassen!)\n",
    "        data_point_id = dim.get(\"dataPointId\") or dim.get(\"id\")\n",
    "        period = dim.get(\"period\") or dim.get(\"year\")\n",
    "        indicator = dim.get(\"indicator\") or dim.get(\"name\") or \"Unknown\"\n",
    "        \n",
    "        if not data_point_id:\n",
    "            print(f\"‚ö†Ô∏è  [{i}/{len(dimensions)}] √úberspringe: Keine data_point_id\")\n",
    "            stats[\"failed\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Duplicate-Check\n",
    "        hash_key = generate_hash(company_id, data_point_id, str(period) if period else None)\n",
    "        if hash_key in seen_hashes:\n",
    "            stats[\"skipped_duplicates\"] += 1\n",
    "            continue\n",
    "        seen_hashes.add(hash_key)\n",
    "        \n",
    "        # Progress\n",
    "        if i % progress_step == 0 or i == 1:\n",
    "            progress_pct = (i / len(dimensions)) * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = i / elapsed if elapsed > 0 else 0\n",
    "            eta = (len(dimensions) - i) / rate if rate > 0 else 0\n",
    "            print(f\"\\nüìä Fortschritt: {i}/{len(dimensions)} ({progress_pct:.1f}%)\")\n",
    "            print(f\"   Erfolg: {stats['success']}, Fehler: {stats['failed']}\")\n",
    "            print(f\"   Rate: {rate:.1f}/s, ETA: {eta:.0f}s\")\n",
    "        \n",
    "        # Parameter f√ºr Request\n",
    "        params = {\n",
    "            \"companyId\": company_id,\n",
    "            \"dataPointId\": data_point_id\n",
    "        }\n",
    "        if period:\n",
    "            params[\"period\"] = period\n",
    "        \n",
    "        # API-Call (mit api_session!)\n",
    "        result = api_session.get(endpoint, params=params, timeout=CONFIG[\"timeout_values\"])\n",
    "        \n",
    "        # Raw-Persistenz (auch Fehler speichern!)\n",
    "        envelope = create_envelope(endpoint, result[\"status\"], params, result[\"data\"])\n",
    "        append_jsonl(values_file, envelope)\n",
    "        \n",
    "        # Statistiken\n",
    "        if result[\"status\"] == 200:\n",
    "            stats[\"success\"] += 1\n",
    "            print(f\"   ‚úÖ [{i}/{len(dimensions)}] {indicator} ({period})\")\n",
    "        else:\n",
    "            stats[\"failed\"] += 1\n",
    "            error_msg = f\"{indicator} ({period}): {result['error']}\"\n",
    "            stats[\"errors\"].append(error_msg)\n",
    "            print(f\"   ‚ùå [{i}/{len(dimensions)}] {error_msg}\")\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä VALUES-ABRUF ABGESCHLOSSEN\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üíæ Daten gespeichert in: {values_file.name}\")\n",
    "    print(f\"\\nüìà Statistiken:\")\n",
    "    print(f\"   Total: {stats['total']}\")\n",
    "    print(f\"   Erfolgreich: {stats['success']}\")\n",
    "    print(f\"   Fehlgeschlagen: {stats['failed']}\")\n",
    "    print(f\"   Duplikate √ºbersprungen: {stats['skipped_duplicates']}\")\n",
    "    print(f\"   Dauer: {elapsed_total:.1f}s\")\n",
    "    print(f\"   Rate: {stats['total']/elapsed_total:.1f} Datenpunkte/s\")\n",
    "    \n",
    "    if stats[\"errors\"] and len(stats[\"errors\"]) <= 10:\n",
    "        print(f\"\\n‚ùå Fehler-Details:\")\n",
    "        for error in stats[\"errors\"][:10]:\n",
    "            print(f\"   - {error}\")\n",
    "    elif stats[\"errors\"]:\n",
    "        print(f\"\\n‚ùå {len(stats['errors'])} Fehler (erste 10 gezeigt)\")\n",
    "        for error in stats[\"errors\"][:10]:\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Values abrufen (nur wenn dimensions vorhanden)\n",
    "# ‚ö†Ô∏è WICHTIG: Nutzt api_session!\n",
    "values_stats = None\n",
    "if dimensions and company_info:\n",
    "    values_stats = fetch_all_values(\n",
    "        api_session,  # <- API Service nutzen!\n",
    "        company_info[\"company_id\"],\n",
    "        dimensions,\n",
    "        CONFIG[\"raw_dir\"]\n",
    "    )\n",
    "    \n",
    "    if values_stats[\"success\"] > 0:\n",
    "        print(\"\\n‚úÖ VALUES ERFOLGREICH ABGERUFEN\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå KEINE VALUES ABGERUFEN\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  √úberspringe Values (keine dimensions)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941878f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. (Optional) Dokumente & Sustainability Reports\n",
    "\n",
    "Zus√§tzlich zu den strukturierten Daten (Metriken) k√∂nnen wir auch **Dokumente** abrufen:\n",
    "- Sustainability Reports (Nachhaltigkeitsberichte)\n",
    "- Annual Reports (Gesch√§ftsberichte)\n",
    "- Weitere PDF/Text-Dokumente\n",
    "\n",
    "### Warum Dokumente?\n",
    "\n",
    "Dokumente enthalten:\n",
    "- ‚úÖ **Textuelle Kontexte** f√ºr AI/Q&A\n",
    "- ‚úÖ **Strategien & Narrative** (nicht nur Zahlen)\n",
    "- ‚úÖ **Qualitative Informationen**\n",
    "- ‚úÖ **Erkl√§rungen** zu den Metriken\n",
    "\n",
    "### Ablauf:\n",
    "\n",
    "1. **Suche** nach Dokumenten f√ºr das Unternehmen\n",
    "2. **Metadaten** abrufen (Titel, Jahr, Typ, URL)\n",
    "3. **Speichern** in `raw/document_search.jsonl`\n",
    "\n",
    "### Hinweis:\n",
    "\n",
    "Der vollst√§ndige Download der PDF-Dateien ist **optional** und nicht Teil dieses MVPs. Wir speichern zun√§chst nur die Metadaten und URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "134b0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÑ SCHRITT 4 (Optional): DOKUMENTE SUCHEN (via Documents-Service)\n",
      "======================================================================\n",
      "Suche nach Dokumenten f√ºr: 'Siemens'\n",
      "\n",
      "üß™ Probe: / mit {'q': 'Siemens'}\n",
      "üîÑ GET / (Versuch 1/3)\n",
      "   URL: https://dataland.com/documents/\n",
      "   Params: {'q': 'Siemens'}\n",
      "‚ùå Client-Fehler (400)\n",
      "   ‚ùå Fehler: 400\n",
      "\n",
      "üß™ Probe: /search mit {'q': 'Siemens'}\n",
      "üîÑ GET /search (Versuch 1/3)\n",
      "   URL: https://dataland.com/documents/search\n",
      "   Params: {'q': 'Siemens'}\n",
      "‚ùå Client-Fehler (404)\n",
      "   ‚ùå Fehler: 404\n",
      "\n",
      "üß™ Probe: / mit {'query': 'Siemens'}\n",
      "üîÑ GET / (Versuch 1/3)\n",
      "   URL: https://dataland.com/documents/\n",
      "   Params: {'query': 'Siemens'}\n",
      "‚ùå Client-Fehler (400)\n",
      "   ‚ùå Fehler: 400\n",
      "\n",
      "üß™ Probe: /search mit {'query': 'Siemens'}\n",
      "üîÑ GET /search (Versuch 1/3)\n",
      "   URL: https://dataland.com/documents/search\n",
      "   Params: {'query': 'Siemens'}\n",
      "‚ùå Client-Fehler (404)\n",
      "   ‚ùå Fehler: 404\n",
      "\n",
      "üß™ Probe: / mit {'q': 'Siemens', 'companyId': 'f16a12ff-714c-4dd1-b141-eb8b0355c833'}\n",
      "üîÑ GET / (Versuch 1/3)\n",
      "   URL: https://dataland.com/documents/\n",
      "   Params: {'q': 'Siemens', 'companyId': 'f16a12ff-714c-4dd1-b141-eb8b0355c833'}\n",
      "‚úÖ Status 200 - Erfolg\n",
      "   ‚úÖ Erfolg! Nutze diesen Endpoint\n",
      "\n",
      "‚úÖ 14 Dokumente gefunden:\n",
      "   1. Untitled (?) - Typ: ?\n",
      "   2. Untitled (2024) - Typ: ?\n",
      "   3. Untitled (2024) - Typ: ?\n",
      "   4. Untitled (?) - Typ: ?\n",
      "   5. Untitled (2022) - Typ: ?\n",
      "   6. Untitled (2024) - Typ: ?\n",
      "   7. Untitled (2024) - Typ: ?\n",
      "   8. Untitled (?) - Typ: ?\n",
      "   9. Untitled (2023) - Typ: ?\n",
      "   10. Untitled (?) - Typ: ?\n",
      "   ... und 4 weitere\n",
      "\n",
      "üìä Statistiken:\n",
      "   Dokumenttypen: {'Unknown': 14}\n",
      "   Jahre: N/A\n",
      "\n",
      "‚úÖ DOKUMENTE GEFUNDEN\n"
     ]
    }
   ],
   "source": [
    "def search_documents(doc_session: DatalandHTTPSession, query: str, \n",
    "                     company_id: Optional[str], raw_dir: Path) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Sucht nach Dokumenten f√ºr ein Unternehmen.\n",
    "    \n",
    "    ‚ö†Ô∏è WICHTIG: Nutzt doc_session (Documents Service), nicht api_session!\n",
    "    \n",
    "    Speichert Suchergebnisse in raw/document_search.jsonl\n",
    "    \n",
    "    Args:\n",
    "        doc_session: DatalandHTTPSession f√ºr Documents-Service\n",
    "        query: Suchbegriff (z.B. \"Siemens\")\n",
    "        company_id: Optional - Unternehmens-ID f√ºr gezielte Suche\n",
    "        raw_dir: Verzeichnis f√ºr Raw-Daten\n",
    "        \n",
    "    Returns:\n",
    "        Liste von Dokument-Metadaten, oder None bei Fehler\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÑ SCHRITT 4 (Optional): DOKUMENTE SUCHEN (via Documents-Service)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Suche nach Dokumenten f√ºr: '{query}'\")\n",
    "    \n",
    "    # Probing-Liste f√ºr Documents\n",
    "    # Da doc_session base_url=\"/documents\" hat, ist \"/\" oder \"/search\" korrekt\n",
    "    candidates = [\n",
    "        (\"/\",       {\"q\": query}),\n",
    "        (\"/search\", {\"q\": query}),\n",
    "        (\"/\",       {\"query\": query}),\n",
    "        (\"/search\", {\"query\": query}),\n",
    "    ]\n",
    "    \n",
    "    # Falls company_id vorhanden, erweitere Parameter\n",
    "    if company_id:\n",
    "        candidates.extend([\n",
    "            (\"/\",       {\"q\": query, \"companyId\": company_id}),\n",
    "            (\"/search\", {\"q\": query, \"companyId\": company_id}),\n",
    "        ])\n",
    "    \n",
    "    successful_result = None\n",
    "    \n",
    "    for endpoint, params in candidates:\n",
    "        print(f\"\\nüß™ Probe: {endpoint} mit {params}\")\n",
    "        \n",
    "        result = doc_session.get(endpoint, params=params, timeout=CONFIG[\"timeout_search\"])\n",
    "        \n",
    "        # Raw-Persistenz\n",
    "        envelope = create_envelope(endpoint, result[\"status\"], params, result[\"data\"])\n",
    "        append_jsonl(raw_dir / \"document_search.jsonl\", envelope)\n",
    "        \n",
    "        if result[\"status\"] == 200 and result[\"data\"]:\n",
    "            print(f\"   ‚úÖ Erfolg! Nutze diesen Endpoint\")\n",
    "            successful_result = result\n",
    "            break\n",
    "        else:\n",
    "            print(f\"   ‚ùå Fehler: {result['status']}\")\n",
    "    \n",
    "    if not successful_result:\n",
    "        print(f\"\\n‚ùå Alle Probing-Versuche fehlgeschlagen\")\n",
    "        print(f\"üíæ Alle Versuche gespeichert in: document_search.jsonl\")\n",
    "        return None\n",
    "    \n",
    "    docs_response = successful_result[\"data\"]\n",
    "    \n",
    "    # Extrahiere Dokumente-Array (Feldname anpassen!)\n",
    "    documents = None\n",
    "    for possible_key in [\"documents\", \"results\", \"data\", \"items\"]:\n",
    "        if possible_key in docs_response:\n",
    "            documents = docs_response[possible_key]\n",
    "            break\n",
    "    \n",
    "    if documents is None and isinstance(docs_response, list):\n",
    "        documents = docs_response\n",
    "    \n",
    "    if not documents:\n",
    "        print(f\"‚ö†Ô∏è  Keine Dokumente gefunden\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(documents)} Dokumente gefunden:\")\n",
    "    \n",
    "    # Zeige erste 10\n",
    "    for i, doc in enumerate(documents[:10], 1):\n",
    "        title = doc.get(\"title\") or doc.get(\"name\") or \"Untitled\"\n",
    "        year = doc.get(\"year\") or doc.get(\"reportingPeriod\") or \"?\"\n",
    "        doc_type = doc.get(\"documentType\") or doc.get(\"type\") or \"?\"\n",
    "        print(f\"   {i}. {title} ({year}) - Typ: {doc_type}\")\n",
    "    \n",
    "    if len(documents) > 10:\n",
    "        print(f\"   ... und {len(documents) - 10} weitere\")\n",
    "    \n",
    "    # Statistiken\n",
    "    types = {}\n",
    "    years = set()\n",
    "    for doc in documents:\n",
    "        doc_type = doc.get(\"documentType\") or doc.get(\"type\") or \"Unknown\"\n",
    "        types[doc_type] = types.get(doc_type, 0) + 1\n",
    "        if \"year\" in doc:\n",
    "            years.add(doc[\"year\"])\n",
    "    \n",
    "    print(f\"\\nüìä Statistiken:\")\n",
    "    print(f\"   Dokumenttypen: {dict(types)}\")\n",
    "    print(f\"   Jahre: {sorted(years) if years else 'N/A'}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# Dokumente suchen (optional)\n",
    "# ‚ö†Ô∏è WICHTIG: Nutzt doc_session!\n",
    "documents = None\n",
    "if company_info:\n",
    "    documents = search_documents(\n",
    "        doc_session,  # <- Documents Service nutzen!\n",
    "        CONFIG[\"company_query\"],\n",
    "        company_info.get(\"company_id\"),\n",
    "        CONFIG[\"raw_dir\"]\n",
    "    )\n",
    "    \n",
    "    if documents:\n",
    "        print(\"\\n‚úÖ DOKUMENTE GEFUNDEN\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  KEINE DOKUMENTE ODER FEHLER\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  √úberspringe Dokumente (keine company_info)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db8f06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Run-Log & Gesamt-Statistiken\n",
    "\n",
    "Zum Abschluss erstellen wir ein **Run-Log**, das alle wichtigen Informationen √ºber diesen Durchlauf speichert.\n",
    "\n",
    "### Was enth√§lt das Run-Log?\n",
    "\n",
    "- ‚úÖ **Zeitstempel** (Start, Ende, Dauer)\n",
    "- ‚úÖ **Konfiguration** (Company Query, Base URL)\n",
    "- ‚úÖ **Statistiken** (Erfolge, Fehler, Records)\n",
    "- ‚úÖ **HTTP-Session-Stats** (Requests, Retries, Rate Limits)\n",
    "- ‚úÖ **Fehler-Zusammenfassung**\n",
    "- ‚úÖ **Compliance-Info** (z.B. API-Nutzungsbedingungen)\n",
    "\n",
    "### Warum ist das wichtig?\n",
    "\n",
    "Das Run-Log erm√∂glicht:\n",
    "- üìä **Monitoring**: Wie gut l√§uft die Pipeline?\n",
    "- üêõ **Debugging**: Was lief schief?\n",
    "- üìù **Audit**: Wann wurden welche Daten gezogen?\n",
    "- ‚öñÔ∏è **Compliance**: Nachweis der regelkonformen Nutzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75caef27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_log\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Run-Log mit BEIDEN Sessions erzeugen\u001b[39;00m\n\u001b[32m     94\u001b[39m run_log = create_run_log(\n\u001b[32m     95\u001b[39m     api_session,  \u001b[38;5;66;03m# <- API Service\u001b[39;00m\n\u001b[32m     96\u001b[39m     doc_session,  \u001b[38;5;66;03m# <- Documents Service\u001b[39;00m\n\u001b[32m     97\u001b[39m     CONFIG[\u001b[33m\"\u001b[39m\u001b[33mraw_dir\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     98\u001b[39m     company_info,\n\u001b[32m     99\u001b[39m     dimensions,\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[43mvalues\u001b[49m,\n\u001b[32m    101\u001b[39m     documents\n\u001b[32m    102\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "def create_run_log(api_session: DatalandHTTPSession, doc_session: DatalandHTTPSession, \n",
    "                   raw_dir: Path, company_info: Optional[Dict], dimensions: Optional[List],\n",
    "                   values: Optional[List], documents: Optional[List]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Erzeugt Run-Log mit Statistiken von BEIDEN Sessions.\n",
    "    \n",
    "    Args:\n",
    "        api_session: DatalandHTTPSession f√ºr API-Service\n",
    "        doc_session: DatalandHTTPSession f√ºr Documents-Service\n",
    "        raw_dir: Verzeichnis f√ºr Raw-Daten\n",
    "        company_info: Unternehmensdaten\n",
    "        dimensions: Dimensionen\n",
    "        values: Datenpunkte\n",
    "        documents: Dokumente\n",
    "        \n",
    "    Returns:\n",
    "        Run-Log als Dictionary\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä SCHRITT 5: RUN-LOG ERZEUGEN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Statistiken von beiden Sessions kombinieren\n",
    "    api_stats = api_session.get_stats()\n",
    "    doc_stats = doc_session.get_stats()\n",
    "    \n",
    "    run_log = {\n",
    "        \"timestamp\": nowz(),\n",
    "        \"company_query\": CONFIG[\"company_query\"],\n",
    "        \"company_found\": bool(company_info),\n",
    "        \"company_id\": company_info.get(\"company_id\") if company_info else None,\n",
    "        \n",
    "        # Gesammelte Daten\n",
    "        \"dimensions_count\": len(dimensions) if dimensions else 0,\n",
    "        \"values_count\": len(values) if values else 0,\n",
    "        \"documents_count\": len(documents) if documents else 0,\n",
    "        \n",
    "        # API Service Statistiken\n",
    "        \"api_service\": {\n",
    "            \"total_requests\": api_stats[\"total_requests\"],\n",
    "            \"successful_requests\": api_stats[\"successful_requests\"],\n",
    "            \"failed_requests\": api_stats[\"failed_requests\"],\n",
    "            \"total_retries\": api_stats[\"total_retries\"],\n",
    "            \"rate_limits_hit\": api_stats[\"rate_limits_hit\"],\n",
    "        },\n",
    "        \n",
    "        # Documents Service Statistiken\n",
    "        \"documents_service\": {\n",
    "            \"total_requests\": doc_stats[\"total_requests\"],\n",
    "            \"successful_requests\": doc_stats[\"successful_requests\"],\n",
    "            \"failed_requests\": doc_stats[\"failed_requests\"],\n",
    "            \"total_retries\": doc_stats[\"total_retries\"],\n",
    "            \"rate_limits_hit\": doc_stats[\"rate_limits_hit\"],\n",
    "        },\n",
    "        \n",
    "        # Gesamt-Statistiken\n",
    "        \"total_statistics\": {\n",
    "            \"total_requests\": api_stats[\"total_requests\"] + doc_stats[\"total_requests\"],\n",
    "            \"successful_requests\": api_stats[\"successful_requests\"] + doc_stats[\"successful_requests\"],\n",
    "            \"failed_requests\": api_stats[\"failed_requests\"] + doc_stats[\"failed_requests\"],\n",
    "            \"total_retries\": api_stats[\"total_retries\"] + doc_stats[\"total_retries\"],\n",
    "            \"rate_limits_hit\": api_stats[\"rate_limits_hit\"] + doc_stats[\"rate_limits_hit\"],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Persistiere\n",
    "    envelope = create_envelope(\"run_log\", 200, {}, run_log)\n",
    "    append_jsonl(raw_dir / \"run_log.jsonl\", envelope)\n",
    "    \n",
    "    print(\"\\n‚úÖ Run-Log gespeichert\")\n",
    "    print(f\"   Dimensionen: {run_log['dimensions_count']}\")\n",
    "    print(f\"   Values: {run_log['values_count']}\")\n",
    "    print(f\"   Dokumente: {run_log['documents_count']}\")\n",
    "    print(f\"\\n   API Service:\")\n",
    "    print(f\"      Requests: {run_log['api_service']['total_requests']} \" +\n",
    "          f\"(‚úÖ {run_log['api_service']['successful_requests']}, \" +\n",
    "          f\"‚ùå {run_log['api_service']['failed_requests']})\")\n",
    "    print(f\"      Retries: {run_log['api_service']['total_retries']}\")\n",
    "    print(f\"      Rate Limits: {run_log['api_service']['rate_limits_hit']}\")\n",
    "    print(f\"\\n   Documents Service:\")\n",
    "    print(f\"      Requests: {run_log['documents_service']['total_requests']} \" +\n",
    "          f\"(‚úÖ {run_log['documents_service']['successful_requests']}, \" +\n",
    "          f\"‚ùå {run_log['documents_service']['failed_requests']})\")\n",
    "    print(f\"      Retries: {run_log['documents_service']['total_retries']}\")\n",
    "    print(f\"      Rate Limits: {run_log['documents_service']['rate_limits_hit']}\")\n",
    "    print(f\"\\n   GESAMT:\")\n",
    "    print(f\"      Total Requests: {run_log['total_statistics']['total_requests']}\")\n",
    "    print(f\"      Retries: {run_log['total_statistics']['total_retries']}\")\n",
    "    \n",
    "    return run_log\n",
    "\n",
    "\n",
    "# Run-Log mit BEIDEN Sessions erzeugen\n",
    "run_log = create_run_log(\n",
    "    api_session,  # <- API Service\n",
    "    doc_session,  # <- Documents Service\n",
    "    CONFIG[\"raw_dir\"],\n",
    "    company_info,\n",
    "    dimensions,\n",
    "    values,\n",
    "    documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf58474",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. HTTP-Session Statistiken & Abschluss\n",
    "\n",
    "Zum Abschluss zeigen wir alle HTTP-Statistiken und eine finale Zusammenfassung.\n",
    "\n",
    "### Was haben wir erreicht? ‚úÖ\n",
    "\n",
    "1. **Company-Resolve**: Aus \"Siemens\" wurde eine eindeutige `company_id`\n",
    "2. **Available Dimensions**: Liste aller verf√ºgbaren Datenpunkte geholt\n",
    "3. **Values**: Systematisch alle Werte abgerufen\n",
    "4. **Documents** (optional): Dokument-Metadaten gesammelt\n",
    "5. **Raw-Persistenz**: Alles in JSONL gespeichert\n",
    "6. **Run-Log**: Vollst√§ndige Dokumentation des Durchlaufs\n",
    "\n",
    "### N√§chste Schritte\n",
    "\n",
    "Im n√§chsten Teil der Pipeline:\n",
    "- **Transform Layer**: Rohdaten ins kanonische Schema √ºberf√ºhren\n",
    "- **Mock-Rooms**: Events strukturiert speichern\n",
    "- **AI & Visualisierung**: Use-Cases implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1006e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Statistiken f√ºr BEIDE Sessions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINALE HTTP STATISTIKEN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\udd35 API SERVICE (https://dataland.com/api)\")\n",
    "print(\"-\" * 70)\n",
    "api_session.print_stats()\n",
    "\n",
    "print(\"\\n\udfe2 DOCUMENTS SERVICE (https://dataland.com/documents)\")\n",
    "print(\"-\" * 70)\n",
    "doc_session.print_stats()\n",
    "\n",
    "# Gesamt-Statistiken\n",
    "api_stats = api_session.get_stats()\n",
    "doc_stats = doc_session.get_stats()\n",
    "total_requests = api_stats[\"total_requests\"] + doc_stats[\"total_requests\"]\n",
    "total_retries = api_stats[\"total_retries\"] + doc_stats[\"total_retries\"]\n",
    "total_rate_limits = api_stats[\"rate_limits_hit\"] + doc_stats[\"rate_limits_hit\"]\n",
    "\n",
    "print(\"\\nüìà GESAMT-STATISTIKEN (beide Services)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total Requests: {total_requests}\")\n",
    "print(f\"Total Retries: {total_retries}\")\n",
    "print(f\"Total Rate Limits: {total_rate_limits}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c338a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé≠ MOCK-DATEN (Entwicklungs-Fallback)\n",
    "\n",
    "Falls **weder API noch Documents** erfolgreich waren, erstellen wir Mock-Daten f√ºr die Entwicklung.\n",
    "\n",
    "### Wann wird Mock-Modus aktiviert?\n",
    "\n",
    "- Kein `DATALAND_TOKEN` in `.env`\n",
    "- Alle Company-Lookup Endpoints fehlgeschlagen (HTTP 404/401)\n",
    "- Network-Probleme (Timeout, Firewall)\n",
    "- Dataland-API offline\n",
    "\n",
    "### Was wird gemockt?\n",
    "\n",
    "1. **Company Info:** Mock-Unternehmen mit fester `company_id`\n",
    "2. **Dimensions:** 6 typische ESG-Dimensionen (CO‚ÇÇ, Energie, Wasser, etc.)\n",
    "3. **Values:** Realistische Werte f√ºr Testdaten\n",
    "4. **Documents:** 2 Mock-Sustainability-Reports\n",
    "\n",
    "### Vorteil:\n",
    "\n",
    "Du kannst die **gesamte Pipeline testen**, ohne Zugang zur echten API. Ideal f√ºr:\n",
    "- Entwicklung ohne API-Zugang\n",
    "- Offline-Arbeit\n",
    "- CI/CD Tests\n",
    "- Demo-Pr√§sentationen\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock-Daten f√ºr Entwicklung (falls echte API nicht verf√ºgbar)\n",
    "def create_mock_data_if_needed():\n",
    "    \"\"\"\n",
    "    Erstellt Mock-Daten, falls keine echten Daten abgerufen wurden.\n",
    "    \"\"\"\n",
    "    global company_info, dimensions, values_stats, documents\n",
    "    \n",
    "    # Pr√ºfe, ob wir Mock-Daten brauchen\n",
    "    need_mock = (\n",
    "        company_info is None or\n",
    "        dimensions is None or\n",
    "        (values_stats and values_stats[\"success\"] == 0)\n",
    "    )\n",
    "    \n",
    "    if not need_mock:\n",
    "        print(\"‚úÖ Echte Daten vorhanden - keine Mock-Daten n√∂tig\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé≠ MOCK-MODUS: Erstelle Testdaten\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Mock Company Info\n",
    "    if company_info is None:\n",
    "        company_info = {\n",
    "            \"company_id\": \"mock_siemens_001\",\n",
    "            \"name\": \"Siemens AG (Mock)\",\n",
    "            \"sector\": \"Industrials\",\n",
    "            \"country\": \"Germany\"\n",
    "        }\n",
    "        print(\"‚úÖ Mock Company Info erstellt\")\n",
    "    \n",
    "    # Mock Dimensions\n",
    "    if dimensions is None:\n",
    "        dimensions = [\n",
    "            {\"dataPointId\": \"co2_scope1_2023\", \"indicator\": \"CO2_Scope1\", \n",
    "             \"period\": \"2023\", \"unit\": \"Mt\"},\n",
    "            {\"dataPointId\": \"co2_scope2_2023\", \"indicator\": \"CO2_Scope2\",\n",
    "             \"period\": \"2023\", \"unit\": \"Mt\"},\n",
    "            {\"dataPointId\": \"energy_2023\", \"indicator\": \"Energy_Consumption\",\n",
    "             \"period\": \"2023\", \"unit\": \"TWh\"},\n",
    "            {\"dataPointId\": \"renewable_2023\", \"indicator\": \"Renewable_Energy_Share\",\n",
    "             \"period\": \"2023\", \"unit\": \"%\"},\n",
    "            {\"dataPointId\": \"water_2023\", \"indicator\": \"Water_Consumption\",\n",
    "             \"period\": \"2023\", \"unit\": \"Mio. m¬≥\"},\n",
    "            {\"dataPointId\": \"waste_2023\", \"indicator\": \"Waste_Recycling_Rate\",\n",
    "             \"period\": \"2023\", \"unit\": \"%\"}\n",
    "        ]\n",
    "        print(f\"‚úÖ Mock Dimensions erstellt ({len(dimensions)} Datenpunkte)\")\n",
    "    \n",
    "    # Mock Values - simuliere erfolgreichen Abruf\n",
    "    if values_stats is None or values_stats[\"success\"] == 0:\n",
    "        # Erstelle Mock-Values-Datei\n",
    "        timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        values_file = CONFIG[\"raw_dir\"] / f\"values_mock_{timestamp}.jsonl\"\n",
    "        \n",
    "        mock_values = [\n",
    "            {\"dataPointId\": \"co2_scope1_2023\", \"value\": 1.24, \"unit\": \"Mt\", \n",
    "             \"quality\": \"verified\"},\n",
    "            {\"dataPointId\": \"co2_scope2_2023\", \"value\": 0.86, \"unit\": \"Mt\",\n",
    "             \"quality\": \"verified\"},\n",
    "            {\"dataPointId\": \"energy_2023\", \"value\": 4.5, \"unit\": \"TWh\",\n",
    "             \"quality\": \"verified\"},\n",
    "            {\"dataPointId\": \"renewable_2023\", \"value\": 70, \"unit\": \"%\",\n",
    "             \"quality\": \"estimated\"},\n",
    "            {\"dataPointId\": \"water_2023\", \"value\": 3.2, \"unit\": \"Mio. m¬≥\",\n",
    "             \"quality\": \"verified\"},\n",
    "            {\"dataPointId\": \"waste_2023\", \"value\": 78, \"unit\": \"%\",\n",
    "             \"quality\": \"verified\"}\n",
    "        ]\n",
    "        \n",
    "        for val in mock_values:\n",
    "            envelope = create_envelope(\n",
    "                \"/data-points/values\",\n",
    "                200,\n",
    "                {\"dataPointId\": val[\"dataPointId\"]},\n",
    "                val\n",
    "            )\n",
    "            append_jsonl(values_file, envelope)\n",
    "        \n",
    "        values_stats = {\n",
    "            \"total\": len(mock_values),\n",
    "            \"success\": len(mock_values),\n",
    "            \"failed\": 0,\n",
    "            \"skipped_duplicates\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "        print(f\"‚úÖ Mock Values erstellt ({len(mock_values)} Werte)\")\n",
    "    \n",
    "    # Mock Documents\n",
    "    if documents is None:\n",
    "        documents = [\n",
    "            {\n",
    "                \"documentId\": \"doc_2023_sus\",\n",
    "                \"title\": \"Sustainability Report 2023\",\n",
    "                \"year\": 2023,\n",
    "                \"documentType\": \"SustainabilityReport\",\n",
    "                \"url\": \"https://example.com/siemens_sus_2023.pdf\"\n",
    "            },\n",
    "            {\n",
    "                \"documentId\": \"doc_2022_sus\",\n",
    "                \"title\": \"Sustainability Report 2022\",\n",
    "                \"year\": 2022,\n",
    "                \"documentType\": \"SustainabilityReport\",\n",
    "                \"url\": \"https://example.com/siemens_sus_2022.pdf\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Speichere in document_search.jsonl\n",
    "        envelope = create_envelope(\n",
    "            \"/documents/search\",\n",
    "            200,\n",
    "            {\"q\": CONFIG[\"company_query\"]},\n",
    "            {\"documents\": documents}\n",
    "        )\n",
    "        append_jsonl(CONFIG[\"raw_dir\"] / \"document_search.jsonl\", envelope)\n",
    "        print(f\"‚úÖ Mock Documents erstellt ({len(documents)} Dokumente)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Mock-Daten-Setup abgeschlossen\")\n",
    "    print(\"   ‚Üí Sie k√∂nnen jetzt mit der Transform-Phase fortfahren\")\n",
    "\n",
    "\n",
    "# Mock-Daten erstellen falls n√∂tig\n",
    "create_mock_data_if_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatalandConnector:\n",
    "    \"\"\"\n",
    "    Connector-Klasse f√ºr die Dataland API.\n",
    "    \n",
    "    Diese Klasse kapselt alle Funktionen, die wir brauchen, um:\n",
    "    - Mit der Dataland API zu kommunizieren\n",
    "    - Daten abzurufen\n",
    "    - Fehler zu behandeln\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialisiert den Connector mit der Konfiguration.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary mit API-Konfiguration (URL, Keys, etc.)\n",
    "        \"\"\"\n",
    "        self.base_url = config[\"base_url\"]\n",
    "        self.api_key = config[\"api_key\"]\n",
    "        self.timeout = config[\"timeout\"]\n",
    "        self.max_retries = config[\"max_retries\"]\n",
    "        \n",
    "        # Session f√ºr effizientere HTTP-Requests (Connection Pooling)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        })\n",
    "        \n",
    "        print(\"‚úÖ DatalandConnector initialisiert\")\n",
    "    \n",
    "    def _make_request(self, endpoint: str, params: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        F√ºhrt einen HTTP-GET Request an einen API-Endpoint aus.\n",
    "        \n",
    "        Diese interne Methode (Pr√§fix _) sollte nicht direkt aufgerufen werden.\n",
    "        Sie wird von den √∂ffentlichen Methoden verwendet.\n",
    "        \n",
    "        Args:\n",
    "            endpoint: Der API-Endpoint (z.B. \"/companies/12345\")\n",
    "            params: Optional - Query-Parameter als Dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mit den API-Response-Daten\n",
    "            \n",
    "        Raises:\n",
    "            Exception: Bei Netzwerk- oder API-Fehlern\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        \n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                print(f\"üîÑ Request an {endpoint} (Versuch {attempt}/{self.max_retries})...\")\n",
    "                \n",
    "                response = self.session.get(\n",
    "                    url,\n",
    "                    params=params,\n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                \n",
    "                # HTTP-Fehler pr√ºfen (4xx, 5xx Status Codes)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                print(f\"‚úÖ Erfolgreiche Antwort (Status {response.status_code})\")\n",
    "                return response.json()\n",
    "                \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è±Ô∏è Timeout bei Versuch {attempt}\")\n",
    "                if attempt == self.max_retries:\n",
    "                    raise Exception(f\"Timeout nach {self.max_retries} Versuchen\")\n",
    "                    \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                print(f\"‚ùå HTTP-Fehler: {e}\")\n",
    "                if response.status_code >= 500 and attempt < self.max_retries:\n",
    "                    print(f\"   Server-Fehler, versuche erneut...\")\n",
    "                    continue\n",
    "                raise\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"‚ùå Netzwerk-Fehler: {e}\")\n",
    "                if attempt == self.max_retries:\n",
    "                    raise\n",
    "    \n",
    "    def get_company_data(self, company_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ruft alle verf√ºgbaren Daten f√ºr ein Unternehmen ab.\n",
    "        \n",
    "        Args:\n",
    "            company_id: Die eindeutige ID des Unternehmens (z.B. ISIN)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mit allen Unternehmensdaten\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä Hole Daten f√ºr Unternehmen: {company_id}\")\n",
    "        endpoint = f\"/companies/{company_id}\"\n",
    "        return self._make_request(endpoint)\n",
    "    \n",
    "    def get_company_reports(self, company_id: str, year: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Ruft Nachhaltigkeitsberichte f√ºr ein Unternehmen ab.\n",
    "        \n",
    "        Args:\n",
    "            company_id: Die eindeutige ID des Unternehmens\n",
    "            year: Optional - Filtert nach einem bestimmten Jahr\n",
    "            \n",
    "        Returns:\n",
    "            Liste von Berichten als Dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìÑ Hole Berichte f√ºr Unternehmen: {company_id}\")\n",
    "        if year:\n",
    "            print(f\"   Gefiltert nach Jahr: {year}\")\n",
    "        \n",
    "        endpoint = f\"/companies/{company_id}/reports\"\n",
    "        params = {\"year\": year} if year else None\n",
    "        \n",
    "        response = self._make_request(endpoint, params)\n",
    "        return response.get(\"reports\", [])\n",
    "    \n",
    "    def get_company_metrics(self, company_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Ruft Nachhaltigkeits-Metriken f√ºr ein Unternehmen ab.\n",
    "        \n",
    "        Metriken sind quantitative Werte wie:\n",
    "        - CO‚ÇÇ-Emissionen\n",
    "        - Energieverbrauch\n",
    "        - Wasserverbrauch\n",
    "        - etc.\n",
    "        \n",
    "        Args:\n",
    "            company_id: Die eindeutige ID des Unternehmens\n",
    "            \n",
    "        Returns:\n",
    "            Liste von Metriken als Dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìà Hole Metriken f√ºr Unternehmen: {company_id}\")\n",
    "        endpoint = f\"/companies/{company_id}/metrics\"\n",
    "        \n",
    "        response = self._make_request(endpoint)\n",
    "        return response.get(\"metrics\", [])\n",
    "\n",
    "print(\"‚úÖ DatalandConnector-Klasse definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32532c0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Connector initialisieren und testen\n",
    "\n",
    "Jetzt erstellen wir eine Instanz unseres Connectors und testen die Verbindung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connector-Instanz erstellen\n",
    "connector = DatalandConnector(CONFIG)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Connector ist bereit f√ºr API-Calls!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099bf4a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Mock-Daten f√ºr Entwicklung und Tests\n",
    "\n",
    "**Wichtiger Hinweis:** Da wir m√∂glicherweise noch keinen echten Dataland-API-Zugang haben, erstellen wir Mock-Daten, die realistisch sind.\n",
    "\n",
    "### Warum Mock-Daten?\n",
    "\n",
    "Mock-Daten erlauben es uns:\n",
    "- **Ohne API-Zugang** zu entwickeln\n",
    "- **Reproduzierbare Tests** durchzuf√ºhren\n",
    "- Die **Pipeline-Logik** zu testen, ohne auf externe Systeme angewiesen zu sein\n",
    "- **Kosten zu sparen** (viele APIs sind kostenpflichtig)\n",
    "\n",
    "Sp√§ter k√∂nnen wir die Mock-Daten einfach durch echte API-Calls ersetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_dataland_response() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Erstellt Mock-Daten, die einer echten Dataland-API-Antwort √§hneln.\n",
    "    \n",
    "    Diese Funktion simuliert, was die Dataland API zur√ºckgeben w√ºrde.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mit simulierten Unternehmensdaten\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"company\": {\n",
    "            \"id\": \"DE0007236101\",\n",
    "            \"name\": \"Siemens AG\",\n",
    "            \"sector\": \"Industrials\",\n",
    "            \"country\": \"Germany\",\n",
    "            \"website\": \"https://www.siemens.com\"\n",
    "        },\n",
    "        \"reports\": [\n",
    "            {\n",
    "                \"id\": \"rep_2023_001\",\n",
    "                \"year\": 2023,\n",
    "                \"title\": \"Sustainability Report 2023\",\n",
    "                \"url\": \"https://dataland.com/reports/siemens-2023\",\n",
    "                \"sections\": [\n",
    "                    {\n",
    "                        \"section\": \"Klimastrategie\",\n",
    "                        \"text\": \"Siemens hat sich verpflichtet, bis 2030 klimaneutral zu werden. Das Unternehmen reduziert kontinuierlich seine CO‚ÇÇ-Emissionen und investiert massiv in erneuerbare Energien.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"section\": \"Emissionsreduktion\",\n",
    "                        \"text\": \"Im Gesch√§ftsjahr 2023 konnte Siemens seine Scope-1- und Scope-2-Emissionen um 15% im Vergleich zum Vorjahr reduzieren. Dies wurde durch Effizienzsteigerungen und den Wechsel zu erneuerbaren Energien erreicht.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"section\": \"Circular Economy\",\n",
    "                        \"text\": \"Siemens f√∂rdert die Kreislaufwirtschaft durch Produktdesign, das Recycling und Wiederverwendung erleichtert. 78% der Produktionsabf√§lle werden recycelt.\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"rep_2022_001\",\n",
    "                \"year\": 2022,\n",
    "                \"title\": \"Sustainability Report 2022\",\n",
    "                \"url\": \"https://dataland.com/reports/siemens-2022\",\n",
    "                \"sections\": [\n",
    "                    {\n",
    "                        \"section\": \"Energieverbrauch\",\n",
    "                        \"text\": \"Der Energieverbrauch von Siemens betrug 2022 insgesamt 4,2 TWh, wovon 65% aus erneuerbaren Quellen stammten.\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"id\": \"met_001\",\n",
    "                \"indicator\": \"CO2_Scope1\",\n",
    "                \"name\": \"CO‚ÇÇ-Emissionen Scope 1\",\n",
    "                \"value\": 1.24,\n",
    "                \"unit\": \"Mt\",\n",
    "                \"period\": \"2023\",\n",
    "                \"description\": \"Direkte Emissionen aus eigenen oder kontrollierten Quellen\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"met_002\",\n",
    "                \"indicator\": \"CO2_Scope2\",\n",
    "                \"name\": \"CO‚ÇÇ-Emissionen Scope 2\",\n",
    "                \"value\": 0.86,\n",
    "                \"unit\": \"Mt\",\n",
    "                \"period\": \"2023\",\n",
    "                \"description\": \"Indirekte Emissionen aus eingekaufter Energie\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"met_003\",\n",
    "                \"indicator\": \"Energy_Consumption\",\n",
    "                \"name\": \"Gesamtenergieverbrauch\",\n",
    "                \"value\": 4.5,\n",
    "                \"unit\": \"TWh\",\n",
    "                \"period\": \"2023\",\n",
    "                \"description\": \"Gesamter Energieverbrauch aller Standorte\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"met_004\",\n",
    "                \"indicator\": \"Renewable_Energy_Share\",\n",
    "                \"name\": \"Anteil erneuerbarer Energien\",\n",
    "                \"value\": 70,\n",
    "                \"unit\": \"%\",\n",
    "                \"period\": \"2023\",\n",
    "                \"description\": \"Anteil erneuerbarer Energien am Gesamtverbrauch\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"met_005\",\n",
    "                \"indicator\": \"Water_Consumption\",\n",
    "                \"name\": \"Wasserverbrauch\",\n",
    "                \"value\": 3.2,\n",
    "                \"unit\": \"Mio. m¬≥\",\n",
    "                \"period\": \"2023\",\n",
    "                \"description\": \"Gesamter Frischwasserverbrauch\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"met_006\",\n",
    "                \"indicator\": \"Waste_Recycling_Rate\",\n",
    "                \"name\": \"Recyclingquote Abfall\",\n",
    "                \"value\": 78,\n",
    "                \"unit\": \"%\",\n",
    "                \"period\": \"2023\",\n",
    "                \"description\": \"Anteil recycelter Produktionsabf√§lle\"\n",
    "            }\n",
    "        ],\n",
    "        \"metadata\": {\n",
    "            \"source\": \"Dataland\",\n",
    "            \"retrieved_at\": datetime.now().isoformat(),\n",
    "            \"api_version\": \"v1\",\n",
    "            \"data_quality\": \"verified\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Mock-Daten erstellen\n",
    "mock_data = create_mock_dataland_response()\n",
    "\n",
    "print(\"‚úÖ Mock-Daten erstellt\")\n",
    "print(f\"\\nüìä √úbersicht der Mock-Daten:\")\n",
    "print(f\"  - Unternehmen: {mock_data['company']['name']}\")\n",
    "print(f\"  - Anzahl Berichte: {len(mock_data['reports'])}\")\n",
    "print(f\"  - Anzahl Metriken: {len(mock_data['metrics'])}\")\n",
    "print(f\"  - Datenquelle: {mock_data['metadata']['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4f549",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Rohdaten inspizieren\n",
    "\n",
    "Schauen wir uns die Struktur der abgerufenen Daten genauer an. Das ist wichtig, um zu verstehen:\n",
    "- Welche Felder vorhanden sind\n",
    "- Wie die Daten strukturiert sind\n",
    "- Was wir im Transform-Schritt verarbeiten m√ºssen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sch√∂ner formatierter Output der Rohdaten\n",
    "print(\"=\"*80)\n",
    "print(\"üì¶ ROHDATEN VON DATALAND (JSON-Format)\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(mock_data, indent=2, ensure_ascii=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe254318",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Datenqualit√§t pr√ºfen\n",
    "\n",
    "Bevor wir die Daten weiterverarbeiten, sollten wir einige grundlegende Qualit√§tspr√ºfungen durchf√ºhren.\n",
    "\n",
    "### Was pr√ºfen wir?\n",
    "\n",
    "- Sind alle erwarteten Felder vorhanden?\n",
    "- Sind die Datentypen korrekt?\n",
    "- Gibt es fehlende Werte?\n",
    "- Sind die Werte plausibel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataland_response(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    F√ºhrt grundlegende Validierungen der API-Response durch.\n",
    "    \n",
    "    Args:\n",
    "        data: Die zu validierende API-Response\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mit Validierungsergebnissen\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"valid\": True,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    # 1. Pr√ºfe Hauptstruktur\n",
    "    required_keys = [\"company\", \"reports\", \"metrics\", \"metadata\"]\n",
    "    for key in required_keys:\n",
    "        if key not in data:\n",
    "            results[\"errors\"].append(f\"Fehlendes Feld: {key}\")\n",
    "            results[\"valid\"] = False\n",
    "    \n",
    "    # 2. Pr√ºfe Company-Daten\n",
    "    if \"company\" in data:\n",
    "        company = data[\"company\"]\n",
    "        if \"name\" not in company or not company[\"name\"]:\n",
    "            results[\"errors\"].append(\"Firmenname fehlt\")\n",
    "            results[\"valid\"] = False\n",
    "        results[\"summary\"][\"company_name\"] = company.get(\"name\", \"N/A\")\n",
    "    \n",
    "    # 3. Pr√ºfe Berichte\n",
    "    if \"reports\" in data:\n",
    "        reports = data[\"reports\"]\n",
    "        results[\"summary\"][\"report_count\"] = len(reports)\n",
    "        \n",
    "        if len(reports) == 0:\n",
    "            results[\"warnings\"].append(\"Keine Berichte vorhanden\")\n",
    "        \n",
    "        # Pr√ºfe jeden Bericht\n",
    "        for i, report in enumerate(reports):\n",
    "            if \"year\" not in report:\n",
    "                results[\"warnings\"].append(f\"Bericht {i}: Jahr fehlt\")\n",
    "            if \"sections\" not in report or len(report[\"sections\"]) == 0:\n",
    "                results[\"warnings\"].append(f\"Bericht {i}: Keine Sections vorhanden\")\n",
    "    \n",
    "    # 4. Pr√ºfe Metriken\n",
    "    if \"metrics\" in data:\n",
    "        metrics = data[\"metrics\"]\n",
    "        results[\"summary\"][\"metric_count\"] = len(metrics)\n",
    "        \n",
    "        if len(metrics) == 0:\n",
    "            results[\"warnings\"].append(\"Keine Metriken vorhanden\")\n",
    "        \n",
    "        # Pr√ºfe jede Metrik\n",
    "        for i, metric in enumerate(metrics):\n",
    "            required_metric_fields = [\"indicator\", \"value\", \"unit\", \"period\"]\n",
    "            for field in required_metric_fields:\n",
    "                if field not in metric:\n",
    "                    results[\"warnings\"].append(f\"Metrik {i}: Feld '{field}' fehlt\")\n",
    "            \n",
    "            # Pr√ºfe, ob value numerisch ist\n",
    "            if \"value\" in metric:\n",
    "                try:\n",
    "                    float(metric[\"value\"])\n",
    "                except (ValueError, TypeError):\n",
    "                    results[\"errors\"].append(f\"Metrik {i}: value ist nicht numerisch\")\n",
    "                    results[\"valid\"] = False\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Validierung durchf√ºhren\n",
    "validation_results = validate_dataland_response(mock_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç DATENQUALIT√ÑTS-PR√úFUNG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if validation_results[\"valid\"]:\n",
    "    print(\"‚úÖ Daten sind g√ºltig!\\n\")\n",
    "else:\n",
    "    print(\"‚ùå Daten haben Fehler!\\n\")\n",
    "\n",
    "print(\"üìä Zusammenfassung:\")\n",
    "for key, value in validation_results[\"summary\"].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "if validation_results[\"errors\"]:\n",
    "    print(\"\\n‚ùå Fehler:\")\n",
    "    for error in validation_results[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if validation_results[\"warnings\"]:\n",
    "    print(\"\\n‚ö†Ô∏è Warnungen:\")\n",
    "    for warning in validation_results[\"warnings\"]:\n",
    "        print(f\"  - {warning}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133896b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Zusammenfassung: Connector/Ingest\n",
    "\n",
    "### Was haben wir erreicht? ‚úÖ\n",
    "\n",
    "1. **Connector-Klasse erstellt**: Eine professionelle, wiederverwendbare Klasse f√ºr API-Kommunikation\n",
    "2. **Fehlerbehandlung**: Retry-Logik, Timeouts, HTTP-Error-Handling\n",
    "3. **Mock-Daten**: Realistische Testdaten f√ºr die Entwicklung\n",
    "4. **Validierung**: Qualit√§tspr√ºfung der eingehenden Daten\n",
    "\n",
    "### Daten√ºbersicht\n",
    "\n",
    "Wir haben folgende Rohdaten gesammelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sch√∂ne Zusammenfassung ausgeben\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ZUSAMMENFASSUNG DER GESAMMELTEN DATEN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüè¢ Unternehmen: {mock_data['company']['name']}\")\n",
    "print(f\"   Sektor: {mock_data['company']['sector']}\")\n",
    "print(f\"   Land: {mock_data['company']['country']}\")\n",
    "\n",
    "print(f\"\\nüìÑ Berichte: {len(mock_data['reports'])} St√ºck\")\n",
    "for report in mock_data['reports']:\n",
    "    print(f\"   - {report['year']}: {report['title']}\")\n",
    "    print(f\"     Sections: {len(report['sections'])}\")\n",
    "\n",
    "print(f\"\\nüìà Metriken: {len(mock_data['metrics'])} St√ºck\")\n",
    "for metric in mock_data['metrics']:\n",
    "    print(f\"   - {metric['name']}: {metric['value']} {metric['unit']} ({metric['period']})\")\n",
    "\n",
    "print(f\"\\nüîñ Metadaten:\")\n",
    "print(f\"   Quelle: {mock_data['metadata']['source']}\")\n",
    "print(f\"   Abgerufen: {mock_data['metadata']['retrieved_at']}\")\n",
    "print(f\"   API-Version: {mock_data['metadata']['api_version']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CONNECTOR/INGEST ABGESCHLOSSEN\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚û°Ô∏è  N√§chster Schritt: Transform Layer (Daten ins kanonische Schema √ºberf√ºhren)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc73d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Company-Lookup: Robuste Fallback-Strategie implementiert!\n",
    "\n",
    "### üéØ Implementierte L√∂sung\n",
    "\n",
    "Da die Dataland-Swagger **keinen eindeutigen Company-Search-Endpoint** dokumentiert, haben wir eine **dreistufige Fallback-Strategie** implementiert:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Prim√§rweg: API-Service Probing**\n",
    "```python\n",
    "search_company_via_api(api_session, query, raw_dir)\n",
    "```\n",
    "\n",
    "**Probiert systematisch:**\n",
    "- `/metadata/companies/search?q=Siemens`\n",
    "- `/metadata/companies?query=Siemens`\n",
    "- `/companies/search?q=Siemens`\n",
    "- `/companies?name=Siemens`\n",
    "- `/entities/search?q=Siemens`\n",
    "- `/api/companies/search?q=Siemens`\n",
    "\n",
    "**Persistenz:** Alle Versuche ‚Üí `raw/company_lookup_api.jsonl`\n",
    "\n",
    "#### 2Ô∏è‚É£ **Fallback: Documents-Service Probing**\n",
    "```python\n",
    "search_company_via_documents(doc_session, query, raw_dir)\n",
    "```\n",
    "\n",
    "**Probiert systematisch:**\n",
    "- `/?q=Siemens`\n",
    "- `/search?q=Siemens`\n",
    "- `/?query=Siemens`\n",
    "- `/search?query=Siemens`\n",
    "- `/companies/search?q=Siemens`\n",
    "- `/documents/search?q=Siemens`\n",
    "\n",
    "**Persistenz:** Alle Versuche ‚Üí `raw/company_lookup_documents.jsonl`\n",
    "\n",
    "#### 3Ô∏è‚É£ **Last-Resort: Mock-Daten**\n",
    "```python\n",
    "create_mock_data_if_needed()\n",
    "```\n",
    "\n",
    "Falls beide Wege fehlschlagen, aktiviert sich automatisch der Mock-Modus.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Vorteile dieser Architektur\n",
    "\n",
    "| Feature | Beschreibung |\n",
    "|---------|--------------|\n",
    "| **Resilience** | System funktioniert auch bei API-√Ñnderungen |\n",
    "| **Debugging** | Alle Probes in JSONL ‚Üí einfache Fehleranalyse |\n",
    "| **Flexibility** | Neue Endpoints via CONFIG hinzuf√ºgen |\n",
    "| **Transparency** | Klare Logs zeigen, welcher Weg erfolgreich war |\n",
    "| **Offline-Ready** | Mock-Modus f√ºr Entwicklung ohne API |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Swagger-Recherche Ergebnis\n",
    "\n",
    "**API-Swagger:** `https://dataland.com/api/swagger-ui/index.html`\n",
    "- ‚úÖ `/metadata/available-data-dimensions` (dokumentiert)\n",
    "- ‚ùå `/metadata/companies/search` (nicht gefunden)\n",
    "- ‚ùì Company-Lookup-Endpoint unklar\n",
    "\n",
    "**Documents-Swagger:** `https://dataland.com/documents/swagger-ui/index.html`\n",
    "- ‚úÖ Document-Operationen (exists, update, extend)\n",
    "- ‚ùå Expliziter Company-Search (nicht gefunden)\n",
    "\n",
    "**‚Üí Daher: Probing-Ansatz ist die robusteste L√∂sung!**\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ N√§chste Schritte\n",
    "\n",
    "1. **Teste das Notebook:**\n",
    "   ```bash\n",
    "   # Erstelle .env mit Token\n",
    "   echo \"DATALAND_TOKEN=your_token_here\" > .env\n",
    "   \n",
    "   # F√ºhre Notebook aus\n",
    "   jupyter notebook mvp_pipeline.ipynb\n",
    "   ```\n",
    "\n",
    "2. **Analysiere Logs:**\n",
    "   ```bash\n",
    "   # Welcher Endpoint war erfolgreich?\n",
    "   cat raw/company_lookup_api.jsonl | jq '.status'\n",
    "   cat raw/company_lookup_documents.jsonl | jq '.status'\n",
    "   ```\n",
    "\n",
    "3. **Update CONFIG:**\n",
    "   - Falls ein Endpoint erfolgreich ist ‚Üí nur diesen in CONFIG behalten\n",
    "   - Reduziere Probing-Liste auf funktionierende Endpoints\n",
    "\n",
    "4. **Transform-Layer:**\n",
    "   - JSONL ‚Üí MetricEvents & TextEvents\n",
    "   - Mock-Rooms Schema\n",
    "   - Visualisierungen\n",
    "\n",
    "---\n",
    "\n",
    "### üí° F√ºr Produktiv-Umgebung\n",
    "\n",
    "Sobald du den **exakten funktionierenden Endpoint** identifiziert hast:\n",
    "\n",
    "```python\n",
    "# Anstatt 6 Probes:\n",
    "CONFIG[\"company_lookup_api_candidates\"] = [\n",
    "    {\"path\": \"/metadata/companies/search\", \"params\": {\"q\": None}}  # ‚Üê nur dieser\n",
    "]\n",
    "```\n",
    "\n",
    "Das reduziert API-Calls und verbessert Performance!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5e419",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Quick-Test: Welcher Endpoint funktioniert?\n",
    "\n",
    "Nach dem ersten Run kannst du schnell pr√ºfen, welcher Endpoint erfolgreich war:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick-Check: Welcher Endpoint war erfolgreich?\n",
    "import json\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç ENDPOINT-ANALYSE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Pr√ºfe API-Lookup\n",
    "api_lookup_file = CONFIG[\"raw_dir\"] / \"company_lookup_api.jsonl\"\n",
    "if api_lookup_file.exists():\n",
    "    print(\"\\nüìÇ API-Service Probes:\")\n",
    "    with open(api_lookup_file, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            status = entry.get(\"status\", \"?\")\n",
    "            endpoint = entry.get(\"endpoint\", \"?\")\n",
    "            symbol = \"‚úÖ\" if status == 200 else \"‚ùå\"\n",
    "            print(f\"   {symbol} {endpoint} ‚Üí HTTP {status}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Keine API-Lookup Versuche gefunden\")\n",
    "\n",
    "# Pr√ºfe Documents-Lookup\n",
    "doc_lookup_file = CONFIG[\"raw_dir\"] / \"company_lookup_documents.jsonl\"\n",
    "if doc_lookup_file.exists():\n",
    "    print(\"\\nüìÇ Documents-Service Probes:\")\n",
    "    with open(doc_lookup_file, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            status = entry.get(\"status\", \"?\")\n",
    "            endpoint = entry.get(\"endpoint\", \"?\")\n",
    "            symbol = \"‚úÖ\" if status == 200 else \"‚ùå\"\n",
    "            print(f\"   {symbol} {endpoint} ‚Üí HTTP {status}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Keine Documents-Lookup Versuche gefunden\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° Tipp: Erfolgreiche Endpoints (‚úÖ) in CONFIG √ºbernehmen!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f1060",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Architektur-√úbersicht: Company-Lookup mit Fallback\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Start: Company Query 'Siemens'] --> B{API-Service Probing}\n",
    "    \n",
    "    B -->|6 Endpoint-Kandidaten| C[/metadata/companies/search]\n",
    "    B --> D[/metadata/companies]\n",
    "    B --> E[/companies/search]\n",
    "    B --> F[/companies]\n",
    "    B --> G[/entities/search]\n",
    "    B --> H[/api/companies/search]\n",
    "    \n",
    "    C -->|HTTP 200| SUCCESS1[‚úÖ Company ID gefunden]\n",
    "    D -->|HTTP 200| SUCCESS1\n",
    "    E -->|HTTP 200| SUCCESS1\n",
    "    F -->|HTTP 200| SUCCESS1\n",
    "    G -->|HTTP 200| SUCCESS1\n",
    "    H -->|HTTP 200| SUCCESS1\n",
    "    \n",
    "    C -->|HTTP 404/401| FALLBACK{Documents-Service Probing}\n",
    "    D -->|HTTP 404/401| FALLBACK\n",
    "    E -->|HTTP 404/401| FALLBACK\n",
    "    F -->|HTTP 404/401| FALLBACK\n",
    "    G -->|HTTP 404/401| FALLBACK\n",
    "    H -->|HTTP 404/401| FALLBACK\n",
    "    \n",
    "    FALLBACK -->|6 Endpoint-Kandidaten| I[/]\n",
    "    FALLBACK --> J[/search]\n",
    "    FALLBACK --> K[/companies/search]\n",
    "    \n",
    "    I -->|HTTP 200| SUCCESS2[‚úÖ Company ID gefunden]\n",
    "    J -->|HTTP 200| SUCCESS2\n",
    "    K -->|HTTP 200| SUCCESS2\n",
    "    \n",
    "    I -->|HTTP 404/401| MOCK[üé≠ Mock-Modus aktivieren]\n",
    "    J -->|HTTP 404/401| MOCK\n",
    "    K -->|HTTP 404/401| MOCK\n",
    "    \n",
    "    SUCCESS1 --> NEXT[Weiter zu Dimensions]\n",
    "    SUCCESS2 --> NEXT\n",
    "    MOCK --> NEXT\n",
    "    \n",
    "    NEXT --> DIMS[/metadata/available-data-dimensions]\n",
    "    DIMS --> VALUES[/data-points/values]\n",
    "    VALUES --> DOCS[/documents/search]\n",
    "    DOCS --> END[‚úÖ Ingest Complete]\n",
    "    \n",
    "    style SUCCESS1 fill:#90EE90\n",
    "    style SUCCESS2 fill:#90EE90\n",
    "    style MOCK fill:#FFD700\n",
    "    style END fill:#90EE90\n",
    "```\n",
    "\n",
    "**Legende:**\n",
    "- üü¢ **Gr√ºn:** Erfolgreicher Pfad (Company ID gefunden)\n",
    "- üü° **Gelb:** Fallback-Modus (Mock-Daten)\n",
    "- **Alle Probes:** Werden in JSONL persistiert f√ºr Debugging\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
